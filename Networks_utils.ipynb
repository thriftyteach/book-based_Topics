{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719c570-5393-4308-b1ae-cbcb4a012331",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Feb 28: saved all functions in utils.py\n",
    "#addon: SUCCEEDS to remove most frequent words (threshold: 72!)\n",
    "# (copied cell 1!) SUCCESS at printing out the adjacency matrix per book as a means to show an illustrative example for the thesis, done on November 2, 2024\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import contractions\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import utils\n",
    "\n",
    "# Load spaCy model and NLP tools\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = r\"C:\\Users\\emine\\Readings_Listenings\"\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = read_text_files(file_paths)\n",
    "\n",
    "    \n",
    "# Load stopwords and headwords\n",
    "proper_nouns = load_proper_nouns(r\"C:\\Users\\emine\\try_env\\Lists\\mod_proper_nouns_misspelled.csv\")\n",
    "\n",
    "headword_directory = r”C:\\Users\\emine\\try_env\\Lists\\BNC-COCA_Headwords”\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = load_word_list(r\"C:\\Users\\emine\\try_env\\Lists\\BNC-COCA_Headwords\\BNC-COCA_headwords_1000.txt\")\n",
    "headwords_two = load_word_list(r\"C:\\Users\\emine\\try_env\\Lists\\BNC-COCA_Headwords\\BNC-COCA_headwords_2000.txt\")\n",
    "headwords_three = load_word_list(r\"C:\\Users\\emine\\try_env\\Lists\\BNC-COCA_Headwords\\BNC-COCA_headwords_3000.txt\")\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "        \n",
    "# Load stop words and proper nouns\n",
    "proper_nouns = load_proper_nouns(r\"C:\\Users\\emine\\data_descriptions\\mod_proper_nouns.csv\")\n",
    "\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 72\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "\n",
    "# Directory for saving and reading files\n",
    "save_directory = r\"C:\\Users\\emine\\try_env\\snapshots_RL\\10_snapshots_RL\"\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Populate the snapshots with text files for each book and unit\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        # Create the text filename for each book and unit (e.g., 1_1_More.txt)\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        # Add the file path to the snapshots dictionary if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Assuming global_graph is the overall network that includes all nodes from all books\n",
    "global_graph = nx.Graph()\n",
    "global_partition = {}  # Stores community memberships across all books\n",
    "\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "        \n",
    "# Process each book's snapshot\n",
    "for book, file_paths in snapshots.items():\n",
    "    #print(f\"\\nProcessing Book {book}:\")  # Output the book number\n",
    "    #print(f\"Files considered for this snapshot: {file_paths}\")  # Output the list of files\n",
    "    # Initialize the graph for this book\n",
    "    if book not in graphs:\n",
    "        graphs[book] = nx.Graph()\n",
    "\n",
    "    # Collect all unique words across all units for this book\n",
    "    all_unique_words = set()\n",
    "    # Create a dictionary to accumulate co-occurrence counts (for reporting?)\n",
    "    cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Process each text file (unit)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Process the text to get the words\n",
    "        words = filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            print(f\"Warning: No valid words extracted from file {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Build the co-occurrence matrix with a window size of 9\n",
    "        window_size = 9\n",
    "        adj_matrix, unique_words = build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)  # Collect all unique words\n",
    "        \n",
    "        # Accumulate co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "    # Convert all unique words to a sorted list\n",
    "    all_unique_words = sorted(list(all_unique_words))\n",
    "    num_words = len(all_unique_words)\n",
    "    print(f\"\\nBook {book} - Number of unique words (nodes): {num_words}\")\n",
    "\n",
    "    # Create a DataFrame for the book-level adjacency matrix\n",
    "    df_book_adj_matrix = pd.DataFrame(\n",
    "        np.zeros((num_words, num_words), dtype=int),\n",
    "        index=all_unique_words,\n",
    "        columns=all_unique_words\n",
    "    )\n",
    "        \n",
    "    # Fill the DataFrame with co-occurrence counts\n",
    "    for word_i, neighbors in cooccurrence_counts.items():\n",
    "        for word_j, count in neighbors.items():\n",
    "            df_book_adj_matrix.at[word_i, word_j] = count\n",
    "\n",
    "    # Calculate the sum of each row (which corresponds to the total co-occurrence count for each word)\n",
    "    row_sums = df_book_adj_matrix.sum(axis=1)\n",
    "    \n",
    "    # Order the DataFrame by the row sums in descending order\n",
    "    df_book_adj_matrix = df_book_adj_matrix.loc[row_sums.sort_values(ascending=False).index]\n",
    "    \n",
    "    # Also reorder columns to match the row ordering\n",
    "    df_book_adj_matrix = df_book_adj_matrix[df_book_adj_matrix.index]\n",
    "    \n",
    "    # Display a sample of 10 x 10 rows and columns\n",
    "    sample_df = df_book_adj_matrix.iloc[:10, :10]\n",
    "    print(\"Sample of 10 x 10 from the adjacency matrix (ordered by word occurrences):\")\n",
    "    print(sample_df)\n",
    "    \n",
    "    # Proceed with creating the graph from the DataFrame\n",
    "    df_ordered = order_dataframe(df_book_adj_matrix)\n",
    "    edges = adjacency_matrix_df_to_edge_list(df_ordered)\n",
    "    for edge in edges:\n",
    "        graphs[book].add_edge(edge[0], edge[1], weight=edge[2])\n",
    "        global_graph.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "\n",
    "    \n",
    "    # perform community detection\n",
    "    partition = community_louvain.best_partition(graphs[book]) \n",
    "    # Store the partition in the global partition\n",
    "    global_partition.update(partition)\n",
    "    \n",
    "    # Assign community and topics to each node before saving\n",
    "    for node in graphs[book].nodes():\n",
    "        # Assign topic keywords to each node\n",
    "        graphs[book].nodes[node]['topics'] = ', '.join(topic_keywords_dict.get(node, {'other'}))\n",
    "\n",
    "        # Assign community membership to nodes from the global partition\n",
    "        if node in global_partition:\n",
    "            graphs[book].nodes[node]['community'] = global_partition[node]\n",
    "        else:\n",
    "            # If the node has not been assigned globally, we assign a default community\n",
    "            graphs[book].nodes[node]['community'] = 'undefined'\n",
    "\n",
    "        # Assign node shape based on the word lists (list1_words, list2_words, list3_words)\n",
    "        node_shape = assign_node_shape(node, list1_words, list2_words, list3_words)\n",
    "        graphs[book].nodes[node]['shape'] = node_shape  # Add shape as an attribute\n",
    "        \n",
    "        # Add the node and edges to the global graph (to build a global representation)\n",
    "        global_graph.add_node(node, **graphs[book].nodes[node])\n",
    "\n",
    "    #Add edges from bookspecific graph to global graph\n",
    "    global_graph.add_edges_from(graphs[book].edges(data=True))\n",
    "\n",
    "    # Check if the graph has nodes and edges\n",
    "    if graphs[book].number_of_nodes() == 0 or graphs[book].number_of_edges() == 0:\n",
    "        print(f\"Warning: The graph for Book {book} is empty! No nodes or edges.\")\n",
    "    else:\n",
    "        print(f\"Graph for Book {book} has {graphs[book].number_of_nodes()} nodes and {graphs[book].number_of_edges()} edges.\")\n",
    "\n",
    "    # Save the graph for the book once all nodes are processed with their communities\n",
    "    gexf_filename = os.path.join(save_directory, f\"{book}_2024_12_global_good_mod_w9_RL_LESSFREQ.gexf\")\n",
    "    nx.write_gexf(graphs[book], gexf_filename)\n",
    "    print(f\"Graph for Book {book} saved as {gexf_filename}\")\n",
    "\n",
    "    # Calculate network metrics for each snapshot (after community assignment)\n",
    "    degree_centrality = nx.degree_centrality(graphs[book])\n",
    "    betweenness_centrality = nx.betweenness_centrality(graphs[book])\n",
    "    modularity_louvain = community_louvain.modularity(\n",
    "        {n: graphs[book].nodes[n]['community'] for n in graphs[book].nodes()},\n",
    "        graphs[book]\n",
    "    )\n",
    "   \n",
    "    # Print out the metrics for comparison\n",
    "    print(f\"Book {book} - Louvain Modularity: {modularity_louvain:.3f}\")\n",
    "    #print(f\"Book {book} - Manual Modularity: {modularity_manual:.3f}\")\n",
    "    #print(f\"Book {book} - Degree Centrality (top 5 nodes): {sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "    #print(f\"Book {book} - Betweenness Centrality (top 5 nodes): {sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec model (as an example)\n",
    "# Replace with the correct path to your word embeddings model\n",
    "word_vectors = KeyedVectors.load_word2vec_format(r\"C:\\Users\\emine\\Downloads\\GoogleNewsvectorsnegative300.bin\", binary=True)\n",
    "\n",
    "# Function to calculate topic coherence using your adjacency matrix\n",
    "def calculate_topic_coherence(community_words, adj_matrix, word_to_index):\n",
    "    coherence_score = 0\n",
    "    word_pairs = 0\n",
    "    \n",
    "    # Calculate the coherence by summing co-occurrences for all word pairs in the community\n",
    "    for i, word1 in enumerate(community_words):\n",
    "        for j, word2 in enumerate(community_words):\n",
    "            if i != j:\n",
    "                idx1 = word_to_index.get(word1)\n",
    "                idx2 = word_to_index.get(word2)\n",
    "                if idx1 is not None and idx2 is not None:\n",
    "                    coherence_score += adj_matrix[idx1, idx2]\n",
    "                    word_pairs += 1\n",
    "    \n",
    "    # Return average pairwise co-occurrence (topic coherence)\n",
    "    return coherence_score / word_pairs if word_pairs > 0 else 0\n",
    "\n",
    "# Function to calculate semantic similarity using word vectors\n",
    "def calculate_semantic_similarity(community_words, word_vectors):\n",
    "    similarity_score = 0\n",
    "    word_pairs = 0\n",
    "    missing_words = 0 # Track how many words are missing from word_vectors\n",
    "    \n",
    "    for i, word1 in enumerate(community_words):\n",
    "        for j, word2 in enumerate(community_words):\n",
    "            if i != j:\n",
    "                try:\n",
    "                    vec1 = word_vectors[word1]\n",
    "                    vec2 = word_vectors[word2]\n",
    "                    # Calculate cosine similarity between word vectors\n",
    "                    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
    "                    similarity_score += similarity\n",
    "                    word_pairs += 1\n",
    "                except KeyError:\n",
    "                    missing_words += 1\n",
    "                    # Skip words not found in the word vector model\n",
    "                    continue\n",
    "    print(f\"Skipped {missing_words} word pairs due to missing vectors.\")\n",
    "    \n",
    "    # Return average pairwise similarity (semantic similarity)\n",
    "    return similarity_score / word_pairs if word_pairs > 0 else 0\n",
    "\n",
    "# Create a global partition for modularity analysis for the global graph\n",
    "communities_global = defaultdict(list)\n",
    "for node in global_graph.nodes():\n",
    "    if 'community' in global_graph.nodes[node]:\n",
    "        community_id = global_graph.nodes[node]['community']\n",
    "        communities_global[community_id].append(node)\n",
    "    else:\n",
    "        # Assign a default community if none is specified\n",
    "        communities_global['undefined'].append(node)\n",
    "\n",
    "# Prepare data for CSV: word, assigned community, frequency (counts of node)\n",
    "data_for_csv = []\n",
    "\n",
    "for community_id, nodes in communities_global.items():\n",
    "    community_subgraph = global_graph.subgraph(nodes)\n",
    "    # Calculate degree of each node (acting as word frequency)\n",
    "    node_degrees = dict(community_subgraph.degree())\n",
    "\n",
    "    # Add each word (node) to the list with its community and calculated frequency (degree)\n",
    "    for word, degree in node_degrees.items():\n",
    "        data_for_csv.append([word, community_id, degree])\n",
    "        \n",
    "# Now, loop over communities and calculate both coherence and similarity\n",
    "for community_id, community_words in communities_global.items():\n",
    "    # Rebuild the co-occurrence matrix and word_to_index for this community\n",
    "    adj_matrix, unique_words = build_cooccurrence_matrix(community_words, window_size=9)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    \n",
    "    coherence = calculate_topic_coherence(community_words, adj_matrix, word_to_index)\n",
    "    semantic_similarity = calculate_semantic_similarity(community_words, word_vectors)\n",
    "    \n",
    "    print(f\"Community {community_id} - Coherence: {coherence}, Semantic Similarity: {semantic_similarity}\")\n",
    "\n",
    "# Write to CSV file\n",
    "csv_file = '2024_12_community_word_categorization_good_mod_w9_LESSFREQ.csv'\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Word', 'Community', 'Frequency'])\n",
    "\n",
    "    # Write the data\n",
    "    writer.writerows(data_for_csv)\n",
    "\n",
    "print(f\"Data successfully written to {csv_file}\")\n",
    "\n",
    "# Save the global graph for reference\n",
    "save_directory = r\"C:\\Users\\emine\\try_env\\snapshots_RL\\10_snapshots_RL\"\n",
    "global_snapshot_filename = os.path.join(save_directory, \"2024_12_global_good_w9_mod_RL_LESSFREQ.gexf\")\n",
    "nx.write_gexf(global_graph, global_snapshot_filename)\n",
    "print(f\"Global graph saved as {global_snapshot_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb2087-51c9-4fbb-9067-2cde4dea6f3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Feb 28: saved all functions in utils.py and attempts to consolidate file paths in BASE_DIR\n",
    "#addon: SUCCEEDS to remove most frequent words (threshold: 72!)\n",
    "# (copied cell 1!) SUCCESS at printing out the adjacency matrix per book as a means to show an illustrative example for the thesis, done on November 2, 2024\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load spaCy model and NLP tools\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 72\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Populate the snapshots with text files for each book and unit\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        # Create the text filename for each book and unit (e.g., 1_1_More.txt)\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        # Add the file path to the snapshots dictionary if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Assuming global_graph is the overall network that includes all nodes from all books\n",
    "global_graph = nx.Graph()\n",
    "global_partition = {}  # Stores community memberships across all books\n",
    "\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "        \n",
    "# Process each book's snapshot\n",
    "for book, file_paths in snapshots.items():\n",
    "    #print(f\"\\nProcessing Book {book}:\")  # Output the book number\n",
    "    #print(f\"Files considered for this snapshot: {file_paths}\")  # Output the list of files\n",
    "    # Initialize the graph for this book\n",
    "    if book not in graphs:\n",
    "        graphs[book] = nx.Graph()\n",
    "\n",
    "    # Collect all unique words across all units for this book\n",
    "    all_unique_words = set()\n",
    "    # Create a dictionary to accumulate co-occurrence counts (for reporting?)\n",
    "    cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Process each text file (unit)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Process the text to get the words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            print(f\"Warning: No valid words extracted from file {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Build the co-occurrence matrix with a window size of 9\n",
    "        window_size = 9\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)  # Collect all unique words\n",
    "        \n",
    "        # Accumulate co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "    # Convert all unique words to a sorted list\n",
    "    all_unique_words = sorted(list(all_unique_words))\n",
    "    num_words = len(all_unique_words)\n",
    "    print(f\"\\nBook {book} - Number of unique words (nodes): {num_words}\")\n",
    "\n",
    "    # Create a DataFrame for the book-level adjacency matrix\n",
    "    df_book_adj_matrix = pd.DataFrame(\n",
    "        np.zeros((num_words, num_words), dtype=int),\n",
    "        index=all_unique_words,\n",
    "        columns=all_unique_words\n",
    "    )\n",
    "        \n",
    "    # Fill the DataFrame with co-occurrence counts\n",
    "    for word_i, neighbors in cooccurrence_counts.items():\n",
    "        for word_j, count in neighbors.items():\n",
    "            df_book_adj_matrix.at[word_i, word_j] = count\n",
    "\n",
    "    # Calculate the sum of each row (which corresponds to the total co-occurrence count for each word)\n",
    "    row_sums = df_book_adj_matrix.sum(axis=1)\n",
    "    \n",
    "    # Order the DataFrame by the row sums in descending order\n",
    "    df_book_adj_matrix = df_book_adj_matrix.loc[row_sums.sort_values(ascending=False).index]\n",
    "    \n",
    "    # Also reorder columns to match the row ordering\n",
    "    df_book_adj_matrix = df_book_adj_matrix[df_book_adj_matrix.index]\n",
    "    \n",
    "    # Display a sample of 10 x 10 rows and columns\n",
    "    sample_df = df_book_adj_matrix.iloc[:10, :10]\n",
    "    print(\"Sample of 10 x 10 from the adjacency matrix (ordered by word occurrences):\")\n",
    "    print(sample_df)\n",
    "    \n",
    "    # Proceed with creating the graph from the DataFrame\n",
    "    df_ordered = utils.order_dataframe(df_book_adj_matrix)\n",
    "    edges = utils.adjacency_matrix_df_to_edge_list(df_ordered)\n",
    "    for edge in edges:\n",
    "        graphs[book].add_edge(edge[0], edge[1], weight=edge[2])\n",
    "        global_graph.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "\n",
    "    \n",
    "    # perform community detection\n",
    "    partition = community_louvain.best_partition(graphs[book]) \n",
    "    # Store the partition in the global partition\n",
    "    global_partition.update(partition)\n",
    "    \n",
    "    # Assign community and topics to each node before saving\n",
    "    for node in graphs[book].nodes():\n",
    "        # Assign topic keywords to each node\n",
    "        graphs[book].nodes[node]['topics'] = ', '.join(topic_keywords_dict.get(node, {'other'}))\n",
    "\n",
    "        # Assign community membership to nodes from the global partition\n",
    "        if node in global_partition:\n",
    "            graphs[book].nodes[node]['community'] = global_partition[node]\n",
    "        else:\n",
    "            # If the node has not been assigned globally, we assign a default community\n",
    "            graphs[book].nodes[node]['community'] = 'undefined'\n",
    "\n",
    "        # Assign node shape based on the word lists (list1_words, list2_words, list3_words)\n",
    "        node_shape = utils.assign_node_shape(node, list1_words, list2_words, list3_words)\n",
    "        graphs[book].nodes[node]['shape'] = node_shape  # Add shape as an attribute\n",
    "        \n",
    "        # Add the node and edges to the global graph (to build a global representation)\n",
    "        global_graph.add_node(node, **graphs[book].nodes[node])\n",
    "\n",
    "    #Add edges from bookspecific graph to global graph\n",
    "    global_graph.add_edges_from(graphs[book].edges(data=True))\n",
    "\n",
    "    # Check if the graph has nodes and edges\n",
    "    if graphs[book].number_of_nodes() == 0 or graphs[book].number_of_edges() == 0:\n",
    "        print(f\"Warning: The graph for Book {book} is empty! No nodes or edges.\")\n",
    "    else:\n",
    "        print(f\"Graph for Book {book} has {graphs[book].number_of_nodes()} nodes and {graphs[book].number_of_edges()} edges.\")\n",
    "\n",
    "    # Save the graph for the book once all nodes are processed with their communities\n",
    "    gexf_filename = os.path.join(save_directory, f\"{book}_202502_global_good_mod_w9_72threshold.gexf\")\n",
    "    nx.write_gexf(graphs[book], gexf_filename)\n",
    "    print(f\"Graph for Book {book} saved as {gexf_filename}\")\n",
    "\n",
    "    # Calculate network metrics for each snapshot (after community assignment)\n",
    "    degree_centrality = nx.degree_centrality(graphs[book])\n",
    "    betweenness_centrality = nx.betweenness_centrality(graphs[book])\n",
    "    modularity_louvain = community_louvain.modularity(\n",
    "        {n: graphs[book].nodes[n]['community'] for n in graphs[book].nodes()},\n",
    "        graphs[book]\n",
    "    )\n",
    "   \n",
    "    # Print out the metrics for comparison\n",
    "    print(f\"Book {book} - Louvain Modularity: {modularity_louvain:.3f}\")\n",
    "    #print(f\"Book {book} - Manual Modularity: {modularity_manual:.3f}\")\n",
    "    #print(f\"Book {book} - Degree Centrality (top 5 nodes): {sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "    #print(f\"Book {book} - Betweenness Centrality (top 5 nodes): {sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to calculate topic coherence using your adjacency matrix\n",
    "def calculate_topic_coherence(community_words, adj_matrix, word_to_index):\n",
    "    coherence_score = 0\n",
    "    word_pairs = 0\n",
    "    \n",
    "    # Calculate the coherence by summing co-occurrences for all word pairs in the community\n",
    "    for i, word1 in enumerate(community_words):\n",
    "        for j, word2 in enumerate(community_words):\n",
    "            if i != j:\n",
    "                idx1 = word_to_index.get(word1)\n",
    "                idx2 = word_to_index.get(word2)\n",
    "                if idx1 is not None and idx2 is not None:\n",
    "                    coherence_score += adj_matrix[idx1, idx2]\n",
    "                    word_pairs += 1\n",
    "    \n",
    "    # Return average pairwise co-occurrence (topic coherence)\n",
    "    return coherence_score / word_pairs if word_pairs > 0 else 0\n",
    "\n",
    "\n",
    "# Create a global partition for modularity analysis for the global graph\n",
    "communities_global = defaultdict(list)\n",
    "for node in global_graph.nodes():\n",
    "    if 'community' in global_graph.nodes[node]:\n",
    "        community_id = global_graph.nodes[node]['community']\n",
    "        communities_global[community_id].append(node)\n",
    "    else:\n",
    "        # Assign a default community if none is specified\n",
    "        communities_global['undefined'].append(node)\n",
    "\n",
    "# Prepare data for CSV: word, assigned community, frequency (counts of node)\n",
    "data_for_csv = []\n",
    "\n",
    "for community_id, nodes in communities_global.items():\n",
    "    community_subgraph = global_graph.subgraph(nodes)\n",
    "    # Calculate degree of each node (acting as word frequency)\n",
    "    node_degrees = dict(community_subgraph.degree())\n",
    "\n",
    "    # Add each word (node) to the list with its community and calculated frequency (degree)\n",
    "    for word, degree in node_degrees.items():\n",
    "        data_for_csv.append([word, community_id, degree])\n",
    "        \n",
    "# Now, loop over communities and calculate both coherence and similarity\n",
    "for community_id, community_words in communities_global.items():\n",
    "    # Rebuild the co-occurrence matrix and word_to_index for this community\n",
    "    adj_matrix, unique_words = utils.build_cooccurrence_matrix(community_words, window_size=9)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    \n",
    "    coherence = calculate_topic_coherence(community_words, adj_matrix, word_to_index)\n",
    "    semantic_similarity = calculate_semantic_similarity(community_words, word_vectors)\n",
    "    \n",
    "    print(f\"Community {community_id} - Coherence: {coherence}, Semantic Similarity: {semantic_similarity}\")\n",
    "\n",
    "utils.save_cooccurrence_matrix(save_directory, adj_matrix, unique_words, \"cooccurrence_matrix.csv\")\n",
    "\n",
    "# Write to CSV file\n",
    "csv_file = '202502_community_word_categorization.csv'\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Word', 'Community', 'Frequency'])\n",
    "\n",
    "    # Write the data\n",
    "    writer.writerows(data_for_csv)\n",
    "\n",
    "print(f\"Data successfully written to {csv_file}\")\n",
    "\n",
    "# Save the global graph for reference\n",
    "global_snapshot_filename = os.path.join(save_directory, \"202502_global_w9_72_threshold.gexf\")\n",
    "nx.write_gexf(global_graph, global_snapshot_filename)\n",
    "print(f\"Global graph saved as {global_snapshot_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa9263b-66e6-423d-a7dd-2a64354ae9bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 72 times:\n",
      "['number', 'thank', 'get', 'right', 'name', 'eat', 'time', 'day', 'lot', 'food', 'people', 'family', 'big', 'home', 'man', 'old', 'many', 'feel', 'let', 'know', 'problem', 'live', 'boy', 'girl', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'next', 'new', 'start', 'phone', 'book', 'mum', 'dad', 'dream', 'course', 'help', 'want', 'thing', 'see', 'say', 'come', 'hour', 'walk', 'stop', 'watch', 'try', 'tell', 'take', 'friend', 'room', 'work', 'read', 'water', 'leave', 'ask', 'find', 'house', 'look', 'call', 'place', 'happy', 'way', 'party', 'year', 'need', 'child', 'talk', 'make', 'money', 'put', 'last', 'world', 'park', 'happen', 'show', 'woman', 'use', 'story', 'interviewer']\n",
      "🌍 Global Graph Modularity Score: 0.244\n",
      "📖 Book 1 - Modularity Score: 0.272\n",
      "📖 Book 1 - Modularity Score: 0.573\n",
      "📖 Book 1 - Modularity Score: 0.637\n",
      "📖 Book 1 - Modularity Score: 0.645\n",
      "📖 Book 1 - Modularity Score: 0.658\n",
      "📖 Book 1 - Modularity Score: 0.645\n",
      "📖 Book 1 - Modularity Score: 0.665\n",
      "📖 Book 1 - Modularity Score: 0.654\n",
      "📖 Book 1 - Modularity Score: 0.637\n",
      "📖 Book 1 - Modularity Score: 0.598\n",
      "📖 Book 1 - Modularity Score: 0.583\n",
      "📖 Book 1 - Modularity Score: 0.573\n",
      "📖 Book 1 - Modularity Score: 0.560\n",
      "📖 Book 1 - Modularity Score: 0.567\n",
      "📖 Book 1 - Modularity Score: 0.560\n",
      "📖 Book 1 - Modularity Score: 0.560\n",
      "📖 Book 1 - Modularity Score: 0.555\n",
      "📖 Book 1 - Modularity Score: 0.539\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\book_1_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.565\n",
      "📖 Book 2 - Modularity Score: 0.599\n",
      "📖 Book 2 - Modularity Score: 0.595\n",
      "📖 Book 2 - Modularity Score: 0.586\n",
      "📖 Book 2 - Modularity Score: 0.590\n",
      "📖 Book 2 - Modularity Score: 0.571\n",
      "📖 Book 2 - Modularity Score: 0.547\n",
      "📖 Book 2 - Modularity Score: 0.525\n",
      "📖 Book 2 - Modularity Score: 0.524\n",
      "📖 Book 2 - Modularity Score: 0.546\n",
      "📖 Book 2 - Modularity Score: 0.519\n",
      "📖 Book 2 - Modularity Score: 0.504\n",
      "📖 Book 2 - Modularity Score: 0.485\n",
      "📖 Book 2 - Modularity Score: 0.475\n",
      "📖 Book 2 - Modularity Score: 0.476\n",
      "📖 Book 2 - Modularity Score: 0.470\n",
      "📖 Book 2 - Modularity Score: 0.457\n",
      "📖 Book 2 - Modularity Score: 0.455\n",
      "📖 Book 2 - Modularity Score: 0.450\n",
      "📖 Book 2 - Modularity Score: 0.439\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\book_2_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.507\n",
      "📖 Book 3 - Modularity Score: 0.577\n",
      "📖 Book 3 - Modularity Score: 0.535\n",
      "📖 Book 3 - Modularity Score: 0.517\n",
      "📖 Book 3 - Modularity Score: 0.482\n",
      "📖 Book 3 - Modularity Score: 0.463\n",
      "📖 Book 3 - Modularity Score: 0.450\n",
      "📖 Book 3 - Modularity Score: 0.427\n",
      "📖 Book 3 - Modularity Score: 0.411\n",
      "📖 Book 3 - Modularity Score: 0.387\n",
      "📖 Book 3 - Modularity Score: 0.373\n",
      "📖 Book 3 - Modularity Score: 0.360\n",
      "📖 Book 3 - Modularity Score: 0.354\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\book_3_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.623\n",
      "📖 Book 4 - Modularity Score: 0.509\n",
      "📖 Book 4 - Modularity Score: 0.532\n",
      "📖 Book 4 - Modularity Score: 0.526\n",
      "📖 Book 4 - Modularity Score: 0.524\n",
      "📖 Book 4 - Modularity Score: 0.514\n",
      "📖 Book 4 - Modularity Score: 0.486\n",
      "📖 Book 4 - Modularity Score: 0.456\n",
      "📖 Book 4 - Modularity Score: 0.446\n",
      "📖 Book 4 - Modularity Score: 0.436\n",
      "📖 Book 4 - Modularity Score: 0.416\n",
      "📖 Book 4 - Modularity Score: 0.400\n",
      "📖 Book 4 - Modularity Score: 0.386\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\book_4_random_network.gexf\n",
      "Final global graph: 3117 nodes, 101067 edges.\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202502_global_random_network.gexf\n",
      "File saved as C:\\Users\\emine\\try_env\\2025_02\\random_global_topic_assignments.csv\n",
      "Global topic assignments saved to C:\\Users\\emine\\try_env\\2025_02\\random_global_topic_assignments.csv.\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# attempts to create one global network\n",
    "\n",
    "# Feb 28: saved all functions in utils.py and attempts to consolidate file paths in BASE_DIR\n",
    "#addon: SUCCEEDS to remove most frequent words (threshold: 72!)\n",
    "# (copied cell 1!) SUCCESS at printing out the adjacency matrix per book as a means to show an illustrative example for the thesis, done on November 2, 2024\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 72\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Define book snapshots before processing\n",
    "snapshots = {}\n",
    "\n",
    "modularity_scores = {}  # Initialize empty dictionary to store modularity scores\n",
    "\n",
    "\n",
    "# Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "\n",
    "# Create a single global co-occurrence matrix\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 9\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "        \n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:  # Prevent self-loops\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "# Build global graph\n",
    "global_graph = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word)\n",
    "\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "\n",
    "# Compute modularity score for the global topic model\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "       \n",
    "# Process books individually\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=9)\n",
    "\n",
    "        # Assign global topic labels\n",
    "        for word in unique_words:\n",
    "            book_graph.add_node(word, topic=word_to_topic.get(word, \"unknown\"))\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        # Compute modularity score for each book-specific graph\n",
    "        book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "        book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "        \n",
    "        print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "        \n",
    "        # Store modularity scores for later analysis\n",
    "        modularity_scores[book] = book_modularity\n",
    "\n",
    "    graphs[book] = book_graph\n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"book_{book}_random_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "    # **Add book-specific graph to global graph**\n",
    "    global_graph.add_edges_from(book_graph.edges(data=True))\n",
    "\n",
    "    # Add book-specific grrah to global graph    \n",
    "    global_graph.add_edges_from(book_graph.edges(data=True))\n",
    "\n",
    "print(f\"Final global graph: {global_graph.number_of_nodes()} nodes, {global_graph.number_of_edges()} edges.\")\n",
    "\n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202502_global_random_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Save global topic assignments\n",
    "output_file = os.path.join(BASE_DIR, \"random_global_topic_assignments.csv\")\n",
    "utils.save_output_file(BASE_DIR, \"random_global_topic_assignments.csv\", \"\\n\".join([f\"{w},{t}\" for w, t in word_to_topic.items()]))\n",
    "\n",
    "print(f\"Global topic assignments saved to {output_file}.\")\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "    \n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db41ceb8-c8d8-4b12-8c8f-220ab9f3e9a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 72 times:\n",
      "['number', 'thank', 'get', 'right', 'name', 'eat', 'time', 'day', 'lot', 'food', 'people', 'family', 'big', 'home', 'man', 'old', 'many', 'feel', 'let', 'know', 'problem', 'live', 'boy', 'girl', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'next', 'new', 'start', 'phone', 'book', 'mum', 'dad', 'dream', 'course', 'help', 'want', 'thing', 'see', 'say', 'come', 'hour', 'walk', 'stop', 'watch', 'try', 'tell', 'take', 'friend', 'room', 'work', 'read', 'water', 'leave', 'ask', 'find', 'house', 'look', 'call', 'place', 'happy', 'way', 'party', 'year', 'need', 'child', 'talk', 'make', 'money', 'put', 'last', 'world', 'park', 'happen', 'show', 'woman', 'use', 'story', 'interviewer']\n",
      "🌍 Global Graph Modularity Score: 0.244\n",
      "📖 Book 1 - Modularity Score: 0.538\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_03_book_1_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.443\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_03_book_2_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.353\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_03_book_3_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.387\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_03_book_4_random_network.gexf\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202503_global_random_network.gexf\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_03_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source to dataframe (first appearance in the book) March 21\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 72\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Define book snapshots before processing\n",
    "snapshots = {}\n",
    "\n",
    "modularity_scores = {}  # Initialize empty dictionary to store modularity scores\n",
    "# Track first appearance of each token\n",
    "token_first_appearance = {}\n",
    "\n",
    "# Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "\n",
    "# Create a single global co-occurrence matrix\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 9\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "\n",
    "        # Track first appearance of each word\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = book\n",
    "        \n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:  # Prevent self-loops\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "# Build global graph\n",
    "global_graph = nx.Graph()\n",
    "# Add nodes\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "\n",
    "# Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Compute modularity score for the global topic model\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "       \n",
    "# Process books individually\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=9)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        # Assign global topic labels\n",
    "        for word in unique_words:\n",
    "            book_graph.add_node(word, topic=word_to_topic.get(word, \"unknown\"), source=token_first_appearance.get(word, book))\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"2025_03_book_{book}_random_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "    \n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202503_global_random_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_03_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e302a37b-6f0c-4e96-a636-e2723b5dfd7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 72 times:\n",
      "['number', 'thank', 'get', 'right', 'name', 'eat', 'time', 'day', 'lot', 'food', 'people', 'family', 'big', 'home', 'man', 'old', 'many', 'feel', 'let', 'know', 'problem', 'live', 'boy', 'girl', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'next', 'new', 'start', 'phone', 'book', 'mum', 'dad', 'dream', 'course', 'help', 'want', 'thing', 'see', 'say', 'come', 'hour', 'walk', 'stop', 'watch', 'try', 'tell', 'take', 'friend', 'room', 'work', 'read', 'water', 'leave', 'ask', 'find', 'house', 'look', 'call', 'place', 'happy', 'way', 'party', 'year', 'need', 'child', 'talk', 'make', 'money', 'put', 'last', 'world', 'park', 'happen', 'show', 'woman', 'use', 'story', 'interviewer']\n",
      "🌍 Global Graph Modularity Score: 0.327\n",
      "📖 Book 1 - Modularity Score: 0.538\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w2_book_1_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.440\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w2_book_2_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.354\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w2_book_3_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.387\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w2_book_4_random_network.gexf\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202504_w2_global_random_network.gexf\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_04_w2_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source to dataframe (first appearance in the book) March 21\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 72\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Define book snapshots before processing\n",
    "snapshots = {}\n",
    "\n",
    "modularity_scores = {}  # Initialize empty dictionary to store modularity scores\n",
    "# Track first appearance of each token\n",
    "token_first_appearance = {}\n",
    "\n",
    "# Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "\n",
    "# Create a single global co-occurrence matrix\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 2\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "\n",
    "        # Track first appearance of each word\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = book\n",
    "        \n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:  # Prevent self-loops\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "# Build global graph\n",
    "global_graph = nx.Graph()\n",
    "# Add nodes\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "\n",
    "# Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Compute modularity score for the global topic model\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "       \n",
    "# Process books individually\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=2)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        # Assign global topic labels\n",
    "        for word in unique_words:\n",
    "            book_graph.add_node(word, topic=word_to_topic.get(word, \"unknown\"), source=token_first_appearance.get(word, book))\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"2025_04_w2_book_{book}_random_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "    \n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202504_w2_global_random_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_04_w2_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e6cb81-29ab-4689-9fa6-c08487935163",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 120 times:\n",
      "['get', 'eat', 'time', 'day', 'lot', 'people', 'big', 'man', 'many', 'let', 'know', 'live', 'like', 'give', 'play', 'school', 'good', 'great', 'think', 'start', 'book', 'mum', 'dad', 'help', 'want', 'thing', 'see', 'say', 'come', 'walk', 'tell', 'take', 'friend', 'work', 'ask', 'find', 'look', 'call', 'year', 'make']\n",
      "🌍 Global Graph Modularity Score: 0.244\n",
      "📖 Book 1 - Modularity Score: 0.523\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_1_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.407\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_2_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.350\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_3_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.377\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_4_random_network.gexf\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202504_w3_global_random_network.gexf\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_04_w3_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 120\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Define book snapshots before processing\n",
    "snapshots = {}\n",
    "\n",
    "modularity_scores = {}  # Initialize empty dictionary to store modularity scores\n",
    "# Track first appearance of each token\n",
    "token_first_appearance = {}\n",
    "\n",
    "# Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "\n",
    "# Create a single global co-occurrence matrix\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "# Build global graph\n",
    "global_graph = nx.Graph()\n",
    "# Add nodes\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "\n",
    "# Add dynamic 'start' time for Gephi timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        global_graph.nodes[word]['start'] = unit_id_to_float(unit_id)\n",
    "\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "\n",
    "# Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Compute modularity score for the global topic model\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "       \n",
    "# Process books individually\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=3)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        \n",
    "        # Assign global topic labels and timeline\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            start = unit_id_to_float(source) if \"_\" in str(source) else None\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=source,\n",
    "                start=start,\n",
    "                end=start + 0.1 if start is not None else None\n",
    "            )\n",
    "\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"2025_04_w3_book_{book}_random_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "    \n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202504_w3_global_random_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_04_w3_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738a24f9-f3ef-473a-aace-df45f3c0495c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 100 times:\n",
      "['get', 'eat', 'time', 'day', 'lot', 'people', 'big', 'man', 'many', 'let', 'know', 'live', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'new', 'start', 'book', 'mum', 'dad', 'help', 'want', 'thing', 'see', 'say', 'come', 'walk', 'watch', 'tell', 'take', 'friend', 'work', 'leave', 'ask', 'find', 'house', 'look', 'call', 'year', 'talk', 'make', 'money', 'world', 'woman']\n",
      "🌍 Global Graph Modularity Score: 0.251\n",
      "📖 Book 1 - Modularity Score: 0.534\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_1__distr_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.419\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_2__distr_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.358\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_3__distr_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.388\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_4__distr_random_network.gexf\n",
      "✅ Exported word_to_topic.json\n",
      "✅ Exported token_counts_per_book.json\n",
      "✅ Exported unit_tokens.json\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202504_w3_global_distr_random_network.gexf\n",
      "📄 Saved unit-level topic coverage CSV to C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_unit_topic_coverage.csv\n",
      "📤 Exported network-based core vocabulary to: analysis_exports/network_core_vocab.json\n",
      "\n",
      "🔍 Topic Assignment Examples:\n",
      "Topic  0: whisper, child, advantage, guest, full, fire, cork, chant, spill, bush\n",
      "Topic  1: wheelchair, rent, ball, badwater, cheap, gig, guide, storey, wing, avoid\n",
      "Topic  2: solve, diamond, signal, pill, drum, previous, translate, found, addict, defend\n",
      "Topic  3: stress, remarkable, organ, fashion, insult, reason, country, climate, sickness, comfort\n",
      "Topic  4: hundred, hem, protestant, expression, version, alp, suggest, medical, outback, care\n",
      "Topic  5: kitchy, bury, real, honeymoon, stamp, society, investigation, scientist, digest, uncommon\n",
      "Topic  6: bright, bike, turn, finish, headphone, cage, fish, cent, motorcycle, guarantee\n",
      "Topic  7: reply, press, forward, starting, seatoll, experienced, polite, finger, rewind, remote\n",
      "Topic  8: brush, fat, thought, hospital, fake, sleepy, pierced, ladder, training, hand\n",
      "Topic  9: stay, grandad, twin, university, aware, epperson, yurt, anorak, wizard, gorilla\n",
      "Topic 10: planning, involve, cabbage, grandmother, ufologist, letter, skill, peer, rich, dreamer\n",
      "Topic 11: hitler, crossed, glass, tidy, tate, hike, victim, railway, unseen, pardon\n",
      "Topic 12: extinct, dangerous, childhood, layer, kilogram, bulb, suck, bacon, use, canoeing\n",
      "Topic 13: cooking, sue, costume, unhealthy, noodle, slipper, bread, maid, machine, pen\n",
      "Topic 14: tea, fowl, topic, adventure, colfer, terrific, game, lonely, coincidence, hobby\n",
      "Topic 15: taunton, focus, tragic, afternoon, mankind, windy, geography, crush, object, heating\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_04_w3_distributional_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "\n",
    "# Directory and pattern to read text files\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "# Collect file paths\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "\n",
    "#Load proper nouns and word lists from centralized directory\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "\n",
    "# Load manually created proper_nouns file and and BNC-COCA headwords' list\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "\n",
    "# Load all three headwords lists\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "\n",
    "\n",
    "# Lemmatize the headword lists to ensure comparison is consistent with the text processing pipeline\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "\n",
    "# Visualization function (same as provided but generalized for more options)\n",
    "def plot_word_frequencies(word_frequencies, top_n=20, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the top N most frequent words.\n",
    "    If log_scale is True, applies a logarithmic scale to the y-axis.\n",
    "    \"\"\"\n",
    "    most_common = word_frequencies.most_common(top_n)\n",
    "    words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words', fontsize=11)\n",
    "    plt.ylabel('Frequencies' if not log_scale else 'Log(Frequencies)', fontsize=14)\n",
    "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=11)\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 60\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "# Define save directory inside BASE_DIR\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "\n",
    "# Collect files per book\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "#Define the file ranges for each book\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "# Define book snapshots before processing\n",
    "snapshots = {}\n",
    "\n",
    "modularity_scores = {}  # Initialize empty dictionary to store modularity scores\n",
    "# Track first appearance of each token\n",
    "token_first_appearance = {}\n",
    "\n",
    "# Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "\n",
    "# Create a single global co-occurrence matrix\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "# Build global graph\n",
    "global_graph = nx.Graph()\n",
    "# Add nodes\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "\n",
    "# Add dynamic 'start' time for Gephi timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        global_graph.nodes[word]['start'] = unit_id_to_float(unit_id)\n",
    "\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "\n",
    "# Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Compute modularity score for the global topic model\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "\n",
    "time_windows = []\n",
    "# First, build a unit_id -> (start, end) lookup based on time_windows\n",
    "unit_interval_map = {}\n",
    "\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "\n",
    "\n",
    "    # Define two time windows per book\n",
    "    time_windows.append((start_unit, midpoint))\n",
    "    time_windows.append((midpoint, end_unit))\n",
    "\n",
    "\n",
    "# Process books individually\n",
    "graphs = {}\n",
    "unit_topic_rows = []\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        # Build unit-level topic distribution based on global topics\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        \n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "        \n",
    "        # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        # Prepare row with topic percentages\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(word_to_topic.values()):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        \n",
    "        unit_topic_rows.append(row)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=3)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        \n",
    "        # Assign global topic labels and timeline\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            start = unit_id_to_float(source) if \"_\" in str(source) else None\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=source,\n",
    "                start=start,\n",
    "                end=start + 0.1 if start is not None else None\n",
    "            )\n",
    "\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"2025_05_w3_book_{book}__network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "export_dir = os.path.join(BASE_DIR, \"analysis_exports\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# ========== DATA CONTAINERS ==========\n",
    "\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)                                # unit_id -> list of tokens\n",
    "\n",
    "# ========== REPROCESS TEXTS FOR EXPORTS ==========\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Get unit ID (e.g., \"1_4\" from \"1_4_More.txt\")\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        # Record words at unit level\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# ========== EXPORT WORD TO TOPIC ==========\n",
    "with open(os.path.join(export_dir, \"word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported word_to_topic.json\")\n",
    "\n",
    "# ========== EXPORT TOKEN COUNTS PER BOOK ==========\n",
    "with open(os.path.join(export_dir, \"token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported token_counts_per_book.json\")\n",
    "\n",
    "# ========== EXPORT UNIT TOKENS ==========\n",
    "with open(os.path.join(export_dir, \"unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported unit_tokens.json\")\n",
    "\n",
    "\n",
    "    \n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202505_w3_global_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "\n",
    "# Export per-unit topic coverage\n",
    "unit_topic_csv = os.path.join(save_directory, \"2025_05_w3_unit_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "\n",
    "# Save to JSON\n",
    "export_path = \"analysis_exports/network_core_vocab.json\"\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(network_core_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "\n",
    "# Collect words by topic\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items(): \n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_05_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988a2507-0440-4c99-a8c3-ad2eb728275c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 100 times:\n",
      "['get', 'eat', 'time', 'day', 'lot', 'people', 'big', 'man', 'many', 'let', 'know', 'live', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'new', 'start', 'book', 'mum', 'dad', 'help', 'want', 'thing', 'see', 'say', 'come', 'walk', 'watch', 'tell', 'take', 'friend', 'work', 'leave', 'ask', 'find', 'house', 'look', 'call', 'year', 'talk', 'make', 'money', 'world', 'woman']\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_1__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_2__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_3__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_4__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_5__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_6__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_7__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_8__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_9__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_10__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_11__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_12__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_13__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_14__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_15__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_16__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_17__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_1_18__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_1__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_2__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_3__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_4__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_5__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_6__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_7__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_8__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_9__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_10__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_11__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_12__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_13__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_14__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_15__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_16__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_17__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_18__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_19__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_2_20__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_1__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_2__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_3__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_4__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_5__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_6__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_7__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_8__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_9__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_10__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_11__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_12__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_3_13__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_1__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_2__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_3__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_4__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_5__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_6__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_7__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_8__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_9__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_10__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_11__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_12__graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2025_04_w3_unit_4_13__graph.gexf\n",
      "🌍 Global Graph Modularity Score: 0.251\n",
      "📖 Book 1 - Modularity Score: 0.543\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_1_metrics_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.418\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_2_metrics_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.356\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_3_metrics_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.389\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_book_4_metrics_network.gexf\n",
      "✅ Exported word_to_topic.json\n",
      "✅ Exported token_counts_per_book.json\n",
      "✅ Exported unit_tokens.json\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\202504_w3_global_metrics_network.gexf\n",
      "📄 Saved unit-level topic coverage CSV to C:\\Users\\emine\\try_env\\2025_02\\snapshots\\2025_04_w3_unit_topic_coverage.csv\n",
      "📤 Exported network-based core vocabulary to: analysis_exports/network_core_vocab.json\n",
      "\n",
      "🔍 Topic Assignment Examples:\n",
      "Topic  0: whisper, child, advantage, guest, full, fire, cork, chant, spill, bush\n",
      "Topic  1: wheelchair, rent, ball, badwater, cheap, gig, guide, storey, wing, avoid\n",
      "Topic  2: solve, diamond, signal, pill, drum, previous, translate, found, addict, defend\n",
      "Topic  3: stress, remarkable, organ, fashion, insult, reason, country, climate, sickness, comfort\n",
      "Topic  4: hundred, hem, protestant, expression, version, alp, suggest, medical, outback, care\n",
      "Topic  5: kitchy, bury, real, honeymoon, stamp, society, investigation, scientist, digest, uncommon\n",
      "Topic  6: bright, bike, turn, finish, headphone, cage, fish, cent, motorcycle, guarantee\n",
      "Topic  7: reply, press, forward, starting, seatoll, experienced, polite, finger, rewind, remote\n",
      "Topic  8: brush, fat, thought, hospital, fake, sleepy, pierced, ladder, training, hand\n",
      "Topic  9: stay, grandad, twin, university, aware, epperson, yurt, anorak, wizard, gorilla\n",
      "Topic 10: planning, involve, cabbage, grandmother, ufologist, letter, skill, peer, rich, dreamer\n",
      "Topic 11: hitler, crossed, glass, tidy, tate, hike, victim, railway, unseen, pardon\n",
      "Topic 12: extinct, dangerous, childhood, layer, kilogram, bulb, suck, bacon, use, canoeing\n",
      "Topic 13: cooking, sue, costume, unhealthy, noodle, slipper, bread, maid, machine, pen\n",
      "Topic 14: tea, fowl, topic, adventure, colfer, terrific, game, lonely, coincidence, hobby\n",
      "Topic 15: taunton, focus, tragic, afternoon, mankind, windy, geography, crush, object, heating\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_04_w3_distributional_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "#seeds random generators for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "\n",
    "#----- other directories to set\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "unit_graphs_dir = os.path.join(BASE_DIR, \"unit_graphs\")\n",
    "os.makedirs(unit_graphs_dir, exist_ok=True)\n",
    "#----- other directories to set\n",
    "\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "\n",
    "#----------- Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 100\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "#----------- Compute word frequencies\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "snapshots = {}\n",
    "graphs = {}\n",
    "\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "    \n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "global_graph = nx.Graph()\n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "\n",
    "#----------- Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        for word in unique_words:\n",
    "            unit_graph.add_node(word, topic=word_to_topic.get(word, \"unknown\"))\n",
    "\n",
    "\n",
    "            \n",
    "# Track first appearance of each token\n",
    "token_first_appearance = {}\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "        \n",
    "        # Build co-occurrence matrix\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        for word in unique_words:\n",
    "            unit_graph.add_node(word, topic=word_to_topic.get(word, \"unknown\"))\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "        # Create unit-level graph\n",
    "        unit_graph = nx.Graph()\n",
    "\n",
    "        # ---- unit-based analysis graphs -----#\n",
    "        # Assign global topic labels and timeline to unit graph\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            start = unit_id_to_float(source) if \"_\" in str(source) else None\n",
    "            unit_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=source,\n",
    "                start=start,\n",
    "                end=start + 0.1 if start is not None else None\n",
    "            )\n",
    "\n",
    "        # Add edges to unit graph\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    unit_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        # Save unit-level graph\n",
    "        unit_gexf_filename = os.path.join(\n",
    "            unit_graphs_dir, f\"2025_04_w3_unit_{unit_id}__graph.gexf\"\n",
    "        )\n",
    "\n",
    "        nx.write_gexf(unit_graph, unit_gexf_filename)\n",
    "        print(f\"📄 Saved unit graph: {unit_gexf_filename}\")\n",
    "\n",
    "# Process each book's text and build a combined co-occurrence matrix\n",
    "unit_graphs = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "#-------------- global graph finalization - constructs global graph, runs community detection, stores topic assignments, computes modularity score\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# Add dynamic 'start' time for Gephi timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        global_graph.nodes[word]['start'] = unit_id_to_float(unit_id)\n",
    "# Add edges\n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "# Add topics to global nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "# modularity score\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "#-------------- global graph finalization - constructs global graph, runs community detection, stores topic assignments, computes modularity score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- unoit-based analysis graphs -----#\n",
    "\n",
    "# --- Define custom time windows for each book ---\n",
    "time_windows = []\n",
    "unit_interval_map = {}\n",
    "\n",
    "for book, units in book_ranges.items():\n",
    "    unit_floats = [unit_id_to_float(f\"{book}_{unit}\") for unit in units]\n",
    "    unit_floats.sort()\n",
    "\n",
    "    if len(unit_floats) == 0:\n",
    "        continue\n",
    "\n",
    "    mid_index = len(unit_floats) // 2\n",
    "    part1 = unit_floats[:mid_index]\n",
    "    part2 = unit_floats[mid_index:]\n",
    "\n",
    "    if part1:\n",
    "        time_windows.append((part1[0], part1[-1]))\n",
    "    if part2:\n",
    "        time_windows.append((part2[0], part2[-1]))\n",
    "\n",
    "# Create lookup from unit_id -> (start, end)\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "\n",
    "# Process books individually\n",
    "graphs = {}\n",
    "unit_topic_rows = []\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        # Build unit-level topic distribution based on global topics\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        \n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "        \n",
    "        # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        # Prepare row with topic percentages\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(word_to_topic.values()):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        \n",
    "        unit_topic_rows.append(row)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=3)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        \n",
    "        # Assign global topic labels and timeline\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            start = unit_id_to_float(source) if \"_\" in str(source) else None\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=source,\n",
    "                start=start,\n",
    "                end=start + 0.1 if start is not None else None\n",
    "            )\n",
    "\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"2025_04_w3_book_{book}_metrics_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "export_dir = os.path.join(BASE_DIR, \"analysis_exports\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# ========== DATA CONTAINERS ==========\n",
    "\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)                                # unit_id -> list of tokens\n",
    "\n",
    "# ========== REPROCESS TEXTS FOR EXPORTS ==========\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Extract filtered words\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Get unit ID (e.g., \"1_4\" from \"1_4_More.txt\")\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        # Record words at unit level\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# ========== EXPORT WORD TO TOPIC ==========\n",
    "with open(os.path.join(export_dir, \"word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported word_to_topic.json\")\n",
    "\n",
    "# ========== EXPORT TOKEN COUNTS PER BOOK ==========\n",
    "with open(os.path.join(export_dir, \"token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported token_counts_per_book.json\")\n",
    "\n",
    "# ========== EXPORT UNIT TOKENS ==========\n",
    "with open(os.path.join(export_dir, \"unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported unit_tokens.json\")\n",
    "\n",
    "\n",
    "    \n",
    "# **Save Global Graph in GEXF Format**\n",
    "global_gexf_filename = os.path.join(save_directory, \"202504_w3_global_metrics_network.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "\n",
    "# Export per-unit topic coverage\n",
    "unit_topic_csv = os.path.join(save_directory, \"2025_04_w3_unit_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "\n",
    "# Save to JSON\n",
    "export_path = \"analysis_exports/network_core_vocab.json\"\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(network_core_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "\n",
    "# Collect words by topic\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items(): \n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "\n",
    "# Save modularity scores\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_04_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff05100d-587f-4389-9380-e9019c232f8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 100 times:\n",
      "['get', 'eat', 'time', 'day', 'lot', 'people', 'big', 'man', 'many', 'let', 'know', 'live', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'think', 'new', 'start', 'book', 'mum', 'dad', 'help', 'want', 'thing', 'see', 'say', 'come', 'walk', 'watch', 'tell', 'take', 'friend', 'work', 'leave', 'ask', 'find', 'house', 'look', 'call', 'year', 'talk', 'make', 'money', 'world', 'woman']\n",
      "Processing unit: 1_1\n",
      "Processing unit: 1_2\n",
      "Processing unit: 1_3\n",
      "Processing unit: 1_4\n",
      "Processing unit: 1_5\n",
      "Processing unit: 1_6\n",
      "Processing unit: 1_7\n",
      "Processing unit: 1_8\n",
      "Processing unit: 1_9\n",
      "Processing unit: 1_10\n",
      "Processing unit: 1_11\n",
      "Processing unit: 1_12\n",
      "Processing unit: 1_13\n",
      "Processing unit: 1_14\n",
      "Processing unit: 1_15\n",
      "Processing unit: 1_16\n",
      "Processing unit: 1_17\n",
      "Processing unit: 1_18\n",
      "Processing unit: 2_1\n",
      "Processing unit: 2_2\n",
      "Processing unit: 2_3\n",
      "Processing unit: 2_4\n",
      "Processing unit: 2_5\n",
      "Processing unit: 2_6\n",
      "Processing unit: 2_7\n",
      "Processing unit: 2_8\n",
      "Processing unit: 2_9\n",
      "Processing unit: 2_10\n",
      "Processing unit: 2_11\n",
      "Processing unit: 2_12\n",
      "Processing unit: 2_13\n",
      "Processing unit: 2_14\n",
      "Processing unit: 2_15\n",
      "Processing unit: 2_16\n",
      "Processing unit: 2_17\n",
      "Processing unit: 2_18\n",
      "Processing unit: 2_19\n",
      "Processing unit: 2_20\n",
      "Processing unit: 3_1\n",
      "Processing unit: 3_2\n",
      "Processing unit: 3_3\n",
      "Processing unit: 3_4\n",
      "Processing unit: 3_5\n",
      "Processing unit: 3_6\n",
      "Processing unit: 3_7\n",
      "Processing unit: 3_8\n",
      "Processing unit: 3_9\n",
      "Processing unit: 3_10\n",
      "Processing unit: 3_11\n",
      "Processing unit: 3_12\n",
      "Processing unit: 3_13\n",
      "Processing unit: 4_1\n",
      "Processing unit: 4_2\n",
      "Processing unit: 4_3\n",
      "Processing unit: 4_4\n",
      "Processing unit: 4_5\n",
      "Processing unit: 4_6\n",
      "Processing unit: 4_7\n",
      "Processing unit: 4_8\n",
      "Processing unit: 4_9\n",
      "Processing unit: 4_10\n",
      "Processing unit: 4_11\n",
      "Processing unit: 4_12\n",
      "Processing unit: 4_13\n",
      "🌍 Global Graph Modularity Score: 0.251\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_1_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_2_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_3_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_4_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_5_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_6_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_7_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_8_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_9_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_10_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_11_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_12_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_13_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_14_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_15_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_16_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_17_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\1_18_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_1_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_2_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_3_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_4_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_5_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_6_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_7_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_8_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_9_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_10_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_11_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_12_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_13_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_14_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_15_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_16_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_17_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_18_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_19_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\2_20_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_1_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_2_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_3_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_4_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_5_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_6_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_7_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_8_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_9_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_10_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_11_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_12_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\3_13_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_1_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_2_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_3_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_4_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_5_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_6_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_7_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_8_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_9_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_10_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_11_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_12_graph.gexf\n",
      "📄 Saved unit graph: C:\\Users\\emine\\try_env\\2025_02\\unit_graphs\\4_13_graph.gexf\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\global_graph_with_topics.gexf\n",
      "📖 Book 1 - Modularity Score: 0.626\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_book_1__distr_random_network.gexf\n",
      "📖 Book 2 - Modularity Score: 0.679\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_book_2__distr_random_network.gexf\n",
      "📖 Book 3 - Modularity Score: 0.529\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_book_3__distr_random_network.gexf\n",
      "📖 Book 4 - Modularity Score: 0.656\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_book_4__distr_random_network.gexf\n",
      "✅ Exported word_to_topic.json\n",
      "✅ Exported 20250425token_counts_per_book.json\n",
      "✅ Exported 20250425unit_tokens.json\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_global_graph.gexf\n",
      "📄 Saved unit-level topic coverage CSV to C:\\Users\\emine\\try_env\\2025_02\\snapshots\\20250425_w3_topic_coverage.csv\n",
      "📤 Exported network-based core vocabulary to: C:\\Users\\emine\\try_env\\2025_02\\analysis_exports\\network_core_vocab.json\n",
      "\n",
      "🔍 Topic Assignment Examples:\n",
      "Topic  0: whisper, child, advantage, guest, full, fire, cork, chant, spill, bush\n",
      "Topic  1: wheelchair, rent, ball, badwater, cheap, gig, guide, storey, wing, avoid\n",
      "Topic  2: solve, diamond, signal, pill, drum, previous, translate, found, addict, defend\n",
      "Topic  3: stress, remarkable, organ, fashion, insult, reason, country, climate, sickness, comfort\n",
      "Topic  4: hundred, hem, protestant, expression, version, alp, suggest, medical, outback, care\n",
      "Topic  5: kitchy, bury, real, honeymoon, stamp, society, investigation, scientist, digest, uncommon\n",
      "Topic  6: bright, bike, turn, finish, headphone, cage, fish, cent, motorcycle, guarantee\n",
      "Topic  7: reply, press, forward, starting, seatoll, experienced, polite, finger, rewind, remote\n",
      "Topic  8: brush, fat, thought, hospital, fake, sleepy, pierced, ladder, training, hand\n",
      "Topic  9: stay, grandad, twin, university, aware, epperson, yurt, anorak, wizard, gorilla\n",
      "Topic 10: planning, involve, cabbage, grandmother, ufologist, letter, skill, peer, rich, dreamer\n",
      "Topic 11: hitler, crossed, glass, tidy, tate, hike, victim, railway, unseen, pardon\n",
      "Topic 12: extinct, dangerous, childhood, layer, kilogram, bulb, suck, bacon, use, canoeing\n",
      "Topic 13: cooking, sue, costume, unhealthy, noodle, slipper, bread, maid, machine, pen\n",
      "Topic 14: tea, fowl, topic, adventure, colfer, terrific, game, lonely, coincidence, hobby\n",
      "Topic 15: taunton, focus, tragic, afternoon, mankind, windy, geography, crush, object, heating\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\2025_0425_w3_distributional_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "\n",
    "#seeds random generators for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "    \n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "export_dir = os.path.join(BASE_DIR, \"analysis_exports\")\n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "\n",
    "#----- other directories to set\n",
    "save_directory = os.path.join(BASE_DIR, \"snapshots\")\n",
    "unit_graphs_dir = os.path.join(BASE_DIR, \"unit_graphs\")\n",
    "os.makedirs(unit_graphs_dir, exist_ok=True)\n",
    "#----- other directories to set\n",
    "\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "\n",
    "\n",
    "#----------- Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 100\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "#----------- Compute word frequencies\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "#----------- # Initialize data structures\n",
    "snapshots = {}  \n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "unit_graphs = defaultdict(dict)\n",
    "token_first_appearance = {}\n",
    "modularity_scores = {}\n",
    "\n",
    "\n",
    "# --- Define custom time windows for each book ---\n",
    "time_windows = []\n",
    "unit_interval_map = {}  # Initialize unit_interval_map\n",
    "unit_topic_rows = []  # For storing topic distribution per unit\n",
    "\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "\n",
    "# Define time windows for each book\n",
    "for book, units in book_ranges.items():\n",
    "    unit_floats = [unit_id_to_float(f\"{book}_{unit}\") for unit in units]\n",
    "    unit_floats.sort()\n",
    "    if unit_floats:\n",
    "        mid_index = len(unit_floats) // 2\n",
    "        part1 = unit_floats[:mid_index]\n",
    "        part2 = unit_floats[mid_index:]\n",
    "        if part1:\n",
    "            time_windows.append((part1[0], part1[-1]))\n",
    "        if part2:\n",
    "            time_windows.append((part2[0], part2[-1]))\n",
    "\n",
    "\n",
    "# Create lookup from unit_id -> (start, end)\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "                \n",
    "# Now `unit_interval_map` is defined and populated\n",
    "\n",
    "\n",
    "#----------- Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "\n",
    "\n",
    "#----------- Unit-level graph construction (process each file/unit)\n",
    "for book, file_paths in snapshots.items():\n",
    "    all_unique_words = set()\n",
    "    for file_path in file_paths:\n",
    "        # Initialize unit graph for each text file\n",
    "        unit_graph = nx.Graph()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Build co-occurrence matrix for unit\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "        all_unique_words.update(unique_words)\n",
    "        \n",
    "         # Get the unit ID from filename (e.g., \"1_4_More.txt\" -> \"1_4\")\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "    \n",
    "        unit_id = \"_\".join(parts[:2])                    # \"1_4\" (book and unit number)\n",
    "        # Check if unit_id was successfully derived\n",
    "        print(f\"Processing unit: {unit_id}\")\n",
    "\n",
    "       # Get the time window for this unit\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "\n",
    "        # Add nodes to the unit-level graph\n",
    "        for word in unique_words:\n",
    "            # Track word's appearance and assign topic and start/end times\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            start = unit_id_to_float(source) if \"_\" in str(source) else None\n",
    "            unit_graph.add_node(\n",
    "                word,\n",
    "                source=source,\n",
    "                unit=unit_id, # Add unit ID as attribute\n",
    "                unit_start=start_float,  # Assign time interval start\n",
    "                unit_end=end_float       # Assign time interval end\n",
    "            )\n",
    "\n",
    "        ## Add edges based on co-occurrence matrix\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    unit_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        unit_graphs[unit_id] = unit_graph  # Store the unit graph\n",
    "    \n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "global_graph = nx.Graph()\n",
    "\n",
    "# Add nodes and edges for global graph\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# add edges \n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Add dynamic 'start' time for Gephi timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        start_val = float(f\"{unit_id_to_float(unit_id):.2f}\")\n",
    "        global_graph.nodes[word]['start'] = f\"{start_val:.2f}\"\n",
    "        global_graph.nodes[word]['end'] = f\"{start_val + 0.1:.2f}\"\n",
    "    else:\n",
    "        global_graph.nodes[word]['start'] = None\n",
    "        global_graph.nodes[word]['end'] = None\n",
    "        \n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "#Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "\n",
    "# modularity score\n",
    "global_modularity = community_louvain.modularity(global_partition, global_graph)\n",
    "print(f\"🌍 Global Graph Modularity Score: {global_modularity:.3f}\")\n",
    "\n",
    "# Now assign topics to the nodes in the unit-level graphs after community detection\n",
    "for unit_id, unit_graph in unit_graphs.items():\n",
    "    for word in unit_graph.nodes:\n",
    "        unit_graph.nodes[word]['topic'] = word_to_topic.get(word, \"unknown\")\n",
    "        # Save the unit graphs after topics are assigned\n",
    "for unit_id, unit_graph in unit_graphs.items():\n",
    "    unit_gexf_filename = os.path.join(unit_graphs_dir, f\"{unit_id}_graph.gexf\")\n",
    "    nx.write_gexf(unit_graph, unit_gexf_filename)\n",
    "    print(f\"📄 Saved unit graph: {unit_gexf_filename}\")\n",
    "    \n",
    "# Save global graph after topics are assigned\n",
    "global_gexf_filename = os.path.join(save_directory, \"global_graph_with_topics.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "#----------------Book-level graph construction (process each book)\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        # Build unit-level topic distribution based on global topics\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        \n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "\n",
    "        # Add dynamic 'start' time for Gephi timeline\n",
    "    for word in global_graph.nodes:\n",
    "        # Get the unit ID from first appearance\n",
    "        unit_id = token_first_appearance.get(word)\n",
    "        \n",
    "        # Get the time interval from `unit_interval_map`\n",
    "        if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "            start_val = float(f\"{unit_id_to_float(unit_id):.2f}\")\n",
    "            global_graph.nodes[word]['start'] = f\"{start_val:.2f}\"\n",
    "            global_graph.nodes[word]['end'] = f\"{start_val + 0.1:.2f}\"\n",
    "        else:\n",
    "            # If no valid unit_id found, you can set the default time window or leave them as None\n",
    "            global_graph.nodes[word]['start'] = None\n",
    "            global_graph.nodes[word]['end'] = None\n",
    "            \n",
    "        # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        # Prepare row with topic percentages\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(word_to_topic.values()):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        \n",
    "        unit_topic_rows.append(row)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size=3)\n",
    "        all_words_in_book.update(unique_words)\n",
    "\n",
    "        \n",
    "        # Assign global topic labels and timeline\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            if \"_\" in str(source):\n",
    "                start_val = float(f\"{unit_id_to_float(source):.2f}\")\n",
    "                start_str = f\"{start_val:.2f}\"\n",
    "                end_str = f\"{start_val + 0.1:.2f}\"\n",
    "            else:\n",
    "                start_str = None\n",
    "                end_str = None\n",
    "        \n",
    "        book_graph.add_node(\n",
    "            word,\n",
    "            topic=word_to_topic.get(word, \"unknown\"),\n",
    "            source=source,\n",
    "            start=start_str,\n",
    "            end=end_str\n",
    "        )\n",
    "\n",
    "        # Add edges\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Compute modularity score for each book-specific graph\n",
    "    book_partition = community_louvain.best_partition(book_graph)  # Detect book-level communities\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    \n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "    \n",
    "    # Store modularity scores for later analysis\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    \n",
    "    #**Save Book Graph in GEXF Format**\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"20250425_book_{book}__distr_random_network.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "# Step 6: Export data\n",
    "# Export word-to-topic mapping\n",
    "with open(os.path.join(export_dir, \"20250425word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported word_to_topic.json\")\n",
    "\n",
    "\n",
    "# 2. Export token counts per book (word frequency for each book)\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)  # unit_id -> list of tokens\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Record words at unit level\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# Export token counts per book\n",
    "with open(os.path.join(export_dir, \"20250425token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 20250425token_counts_per_book.json\")\n",
    "\n",
    "# 3. Export unit tokens (words per unit)\n",
    "with open(os.path.join(export_dir, \"20250425unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 20250425unit_tokens.json\")\n",
    "\n",
    "# Step 6: Export the Global Graph (after all the book-level graphs)\n",
    "global_gexf_filename = os.path.join(save_directory, \"20250425_global_graph.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Export per-unit topic coverage (CSV format)\n",
    "unit_topic_csv = os.path.join(save_directory, \"20250425_w3_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "#print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "\n",
    "# Save to JSON\n",
    "export_path = os.path.join(export_dir, \"network_core_vocab.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(network_core_vocab, f, ensure_ascii=False, indent=2)\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "# Step 7: Collect words by topic and print topic examples\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items():\n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "# Step 8: Save modularity scores (Book-level modularity)\n",
    "modularity_output_file = os.path.join(BASE_DIR, \"2025_0425_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6285cb-b72b-48c5-b13c-71240082688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words occurring more than 60 times:\n",
      "['number', 'thank', 'get', 'right', 'name', 'eat', 'time', 'day', 'lot', 'food', 'people', 'family', 'big', 'home', 'man', 'old', 'many', 'feel', 'let', 'know', 'problem', 'live', 'small', 'boy', 'girl', 'like', 'give', 'play', 'school', 'love', 'good', 'great', 'bad', 'think', 'next', 'new', 'start', 'phone', 'book', 'mum', 'buy', 'dad', 'dream', 'course', 'help', 'night', 'answer', 'want', 'thing', 'little', 'see', 'say', 'hear', 'minute', 'come', 'tree', 'hour', 'walk', 'stop', 'watch', 'try', 'tell', 'sure', 'take', 'car', 'friend', 'room', 'work', 'read', 'water', 'leave', 'ask', 'find', 'house', 'look', 'door', 'call', 'place', 'happy', 'way', 'party', 'year', 'need', 'child', 'talk', 'make', 'kid', 'money', 'put', 'last', 'black', 'parent', 'world', 'climb', 'fall', 'park', 'idea', 'happen', 'believe', 'show', 'woman', 'use', 'job', 'story', 'mean', 'famous', 'wear', 'interviewer', 'allow']\n",
      "📖 Book 1 - Modularity Score: 0.595\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_book_1.gexf\n",
      "📖 Book 2 - Modularity Score: 0.496\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_book_2.gexf\n",
      "📖 Book 3 - Modularity Score: 0.421\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_book_3.gexf\n",
      "📖 Book 4 - Modularity Score: 0.446\n",
      "📂 Saved book graph: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_book_4.gexf\n",
      "✅ Exported w3_word_to_topic.json\n",
      "✅ Exported w3_token_counts_per_book.json\n",
      "✅ Exported w3_unit_tokens.json\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_global_graph.gexf\n",
      "📄 Saved unit-level topic coverage CSV to C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_topic_coverage.csv\n",
      "📤 Exported network-based core vocabulary to: C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_network_core_vocab.json\n",
      "Exported global nodes to C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\w3_global_nodes.json\n",
      "\n",
      "🔍 Topic Assignment Examples:\n",
      "Topic  0: wizard, classmate, arrange, guitar, free, expendable, coordinate, chest, sprinkle, cartoon\n",
      "Topic  1: society, ostrich, balcony, baby, camping, entertainment, faint, religion, specie, attacker\n",
      "Topic  2: stack, favorite, unable, snowboard, packing, futility, picture, superhero, hotel, address\n",
      "Topic  3: entertain, sunny, record, northern, end, homework, rainy, container, cold, sight\n",
      "Topic  4: buffy, halloween, unconscious, gps, rendition, fantastic, tired, badminton, spill, martial\n",
      "Topic  5: patient, cindy, illegal, bookshop, personal, flying, rob, relate, horizon, provide\n",
      "Topic  6: eastern, whiteoak, button, bride, tremble, finger, greet, canoe, casual, laughing\n",
      "Topic  7: french, original, station, knock, credit, lane, jump, donabate, thanksgive, flower\n",
      "Topic  8: prison, twist, physical, pad, bathroom, mysterious, orangutan, communicate, knee, rebel\n",
      "Topic  9: fight, display, progress, needle, guarantee, trade, wildlife, script, eyesight, widescreen\n",
      "Topic 10: order, breakfast, handful, ban, nugget, scene, lamb, bully, grape, waitress\n",
      "Topic 11: seed, tape, driveway, mirror, desk, luggage, spiky, sharpener, rainbow, journal\n",
      "Topic 12: destination, terrific, survivor, climber, blackfoot, crowd, dragon, egyptian, salt, youth\n",
      "Topic 13: block, impress, impossible, save, missionary, plaque, engine, real, activity, territoy\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\w3_exports\\2025_05_w3_distributional_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "# adds unit level networks for stats analysis April 25\n",
    "# adds debugging to network construction April 29\n",
    "# uses the window determined to be best by my hierarchical script May 17\n",
    "\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "\n",
    "#seeds random generators for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "    \n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "export_dir = os.path.join(BASE_DIR, \"w3_exports\")\n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "\n",
    "#----- other directories to set\n",
    "save_directory = os.path.join(BASE_DIR, \"w3_exports\")\n",
    "output_directory = os.path.join(BASE_DIR, \"w3_exports\")\n",
    "unit_graphs_dir = os.path.join(BASE_DIR, \"w3_exports\")\n",
    "os.makedirs(unit_graphs_dir, exist_ok=True)\n",
    "#----- other directories to set\n",
    "\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "\n",
    "\n",
    "#----------- Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 60\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "#----------- Compute word frequencies\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "#----------- # Initialize data structures\n",
    "snapshots = {}  \n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "unit_graphs = defaultdict(dict)\n",
    "token_first_appearance = {}\n",
    "modularity_scores = {}\n",
    "\n",
    "\n",
    "# --- Define custom time windows for each book ---\n",
    "time_windows = []\n",
    "unit_interval_map = {}  # Initialize unit_interval_map\n",
    "unit_topic_rows = []  # For storing topic distribution per unit\n",
    "\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "#----------- Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "\n",
    "# Define time windows for each book\n",
    "for book, units in book_ranges.items():\n",
    "    unit_floats = [unit_id_to_float(f\"{book}_{unit}\") for unit in units]\n",
    "    unit_floats.sort()\n",
    "    if unit_floats:\n",
    "        mid_index = len(unit_floats) // 2\n",
    "        part1 = unit_floats[:mid_index]\n",
    "        part2 = unit_floats[mid_index:]\n",
    "        if part1:\n",
    "            time_windows.append((part1[0], part1[-1]))\n",
    "        if part2:\n",
    "            time_windows.append((part2[0], part2[-1]))\n",
    "\n",
    "\n",
    "# Create lookup from unit_id -> (start, end)\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "                \n",
    "# Now `unit_interval_map` is defined and populated\n",
    "\n",
    "#----------- Unit-level graph construction (process each file/unit)\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        unit_graph = nx.Graph()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        # Add nodes with topic and filename info\n",
    "        for word in unique_words:\n",
    "            unit_graph.add_node(\n",
    "                word,\n",
    "                source=unit_id,\n",
    "                file_name=name_no_ext  # add full file name without extension\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    unit_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        unit_graphs[unit_id] = unit_graph\n",
    "\n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "                    \n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "global_graph = nx.Graph()\n",
    "\n",
    "# Add nodes and edges for global graph\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, file_name=name_no_ext, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# add edges \n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Add dynamic 'start'for timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        start_val = unit_id_to_float(unit_id)\n",
    "        global_graph.nodes[word]['start'] = start_val\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "#Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Define the same palette you used in matplotlib:\n",
    "hex_palette = [\n",
    "    \"#e6194b\", \"#3cb44b\", \"#722dd0\", \"#4363d8\",\n",
    "    \"#f58231\", \"#cc3300\", \"#46f0f0\", \"#0fa9d0\",\n",
    "    \"#384a03\", \"#663300\", \"#008080\", \"#6e00b3\",\n",
    "    \"#9a6324\", \"#0fa929\", \"#800000\", \"#3333ff\",\n",
    "]\n",
    "topic_colors = {tid: hex_palette[tid] for tid in range(len(hex_palette))}\n",
    "\n",
    "def annotate_with_colors(G):\n",
    "    for n, data in G.nodes(data=True):\n",
    "        topic = data.get('topic')\n",
    "        # fallback if something went wrong\n",
    "        hexcol = topic_colors.get(topic, \"#CCCCCC\")\n",
    "        # 2) regular attribute:\n",
    "        G.nodes[n]['color'] = hexcol\n",
    "        # 3) viz:color block for GEXF viewers:\n",
    "        r, g, b = (int(hexcol.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        G.nodes[n]['viz'] = {'color': {'r': r, 'g': g, 'b': b, 'a': 1.0}}\n",
    "\n",
    "# Apply to all your unit‐level graphs:\n",
    "for unit_id, unit_graph in unit_graphs.items():\n",
    "    for word in unit_graph.nodes:\n",
    "        unit_graph.nodes[word]['topic'] = word_to_topic.get(word, \"unkown\")\n",
    "        annotate_with_colors(unit_graph)\n",
    "    nx.write_gexf(unit_graph, os.path.join(unit_graphs_dir, f\"{unit_id}_graph_w3.gexf\"))\n",
    "\n",
    "# …and to the global graph:\n",
    "annotate_with_colors(global_graph)\n",
    "nx.write_gexf(global_graph, os.path.join(output_directory, \"202505_w3_global_metrics_network.gexf\"))\n",
    "\n",
    "\n",
    "\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "# Function to export graph data (nodes, edges) into CSV or JSON\n",
    "def export_graph_data(book_graph, book_name):\n",
    "    # Export nodes (word, topic, source, start time) to a DataFrame\n",
    "    node_data = []\n",
    "    for node, data in book_graph.nodes(data=True):\n",
    "        node_info = {\n",
    "            'word': node,\n",
    "            'topic': data.get('topic', 'unknown'),\n",
    "            'source': data.get('source', 'unknown'),\n",
    "            'start': data.get('start', None)\n",
    "        }\n",
    "        node_data.append(node_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    node_df = pd.DataFrame(node_data)\n",
    "    node_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.csv\"), index=False)\n",
    "    node_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.json\"), orient='records', lines=True)\n",
    "    \n",
    "    # Export edges (word pairs, co-occurrence weight) to a DataFrame\n",
    "    edge_data = []\n",
    "    for u, v, data in book_graph.edges(data=True):\n",
    "        edge_info = {\n",
    "            'word_1': u,\n",
    "            'word_2': v,\n",
    "            'weight': data.get('weight', 0)\n",
    "        }\n",
    "        edge_data.append(edge_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    edge_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_edges.csv\"), index=False)\n",
    "    edge_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_edges.json\"), orient='records', lines=True)\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "\n",
    "# ---------------- Book-level graph construction (process each book)\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Compute topic distribution for the unit\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "\n",
    "       # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(set(word_to_topic.values())):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        unit_topic_rows.append(row)\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            if \"_\" in str(source):\n",
    "                start_val = unit_id_to_float(source)\n",
    "                start = start_val\n",
    "            else:\n",
    "                start = None\n",
    "\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=str(source),\n",
    "                file_name=name_no_ext,\n",
    "                start=start,\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Perform community detection for the book\n",
    "    book_partition = community_louvain.best_partition(book_graph)\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "\n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "\n",
    "    # Annotate with the same colors\n",
    "    annotate_with_colors(book_graph)\n",
    "    \n",
    "    # Store results\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "\n",
    "    # Save book graph\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"w3_book_{book}.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "# Step 6: Export data\n",
    "# Export word-to-topic mapping\n",
    "with open(os.path.join(export_dir, \"w3_word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported w3_word_to_topic.json\")\n",
    "\n",
    "\n",
    "# 2. Export token counts per book (word frequency for each book)\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)  # unit_id -> list of tokens\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Record words at unit level\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# Export token counts per book\n",
    "with open(os.path.join(export_dir, \"w3_token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported w3_token_counts_per_book.json\")\n",
    "\n",
    "# 3. Export unit tokens (words per unit)\n",
    "with open(os.path.join(export_dir, \"w3_unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported w3_unit_tokens.json\")\n",
    "\n",
    "# Step 6: Export the Global Graph (after all the book-level graphs)\n",
    "global_gexf_filename = os.path.join(save_directory, \"w3_global_graph.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Export per-unit topic coverage (CSV format)\n",
    "unit_topic_csv = os.path.join(output_directory, \"w3_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "global_nodes = list(global_graph.nodes)\n",
    "#Count the degree of each node (i.e., how many edges are connected to it)\n",
    "node_degrees = {node: global_graph.degree(node) for node in network_core_vocab}\n",
    "\n",
    "# Filter nodes with degrees greater than 5\n",
    "filtered_nodes = [node for node, degree in node_degrees.items() if degree > 5]\n",
    "\n",
    "# Create a list of nodes with their degrees for the export\n",
    "export_vocab = [\n",
    "    {\"token\": node, \"count\": node_degrees[node]} for node in filtered_nodes\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "export_path = os.path.join(export_dir, \"w3_network_core_vocab.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "#save all nodes to json\n",
    "export_path = os.path.join(export_dir, \"w3_global_nodes.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported global nodes to {export_path}\")\n",
    "\n",
    "# Step 7: Collect words by topic and print topic examples\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items():\n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "# Step 8: Save modularity scores (Book-level modularity)\n",
    "modularity_output_file = os.path.join(output_directory, \"2025_05_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af9f77-9437-4027-bce6-26f53263427a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "# adds unit level networks for stats analysis April 25\n",
    "# adds debugging to network construction April 29\n",
    "# visualizes in pyvis because colors change constantly in GEPHI (May 04)\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "from pyvis.network import Network\n",
    "\n",
    "#seeds random generators for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "    \n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "export_dir = os.path.join(BASE_DIR, \"analysis_exports\")\n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "\n",
    "#----- other directories to set\n",
    "save_directory = os.path.join(BASE_DIR, \"book_graphs\")\n",
    "output_directory = os.path.join(BASE_DIR, \"analysis_exports\")\n",
    "unit_graphs_dir = os.path.join(BASE_DIR, \"unit_graphs\")\n",
    "os.makedirs(unit_graphs_dir, exist_ok=True)\n",
    "#----- other directories to set\n",
    "\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "\n",
    "\n",
    "#----------- Compute word frequencies\n",
    "word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "threshold = 100\n",
    "high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "stop_words.update(high_frequency_words)\n",
    "#----------- Compute word frequencies\n",
    "\n",
    "print(f\"Words occurring more than {threshold} times:\")\n",
    "print(high_frequency_words)\n",
    "\n",
    "#----------- # Initialize data structures\n",
    "snapshots = {}  \n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "unit_graphs = defaultdict(dict)\n",
    "token_first_appearance = {}\n",
    "modularity_scores = {}\n",
    "\n",
    "\n",
    "# --- Define custom time windows for each book ---\n",
    "time_windows = []\n",
    "unit_interval_map = {}  # Initialize unit_interval_map\n",
    "unit_topic_rows = []  # For storing topic distribution per unit\n",
    "\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "#----------- Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "\n",
    "# Define time windows for each book\n",
    "for book, units in book_ranges.items():\n",
    "    unit_floats = [unit_id_to_float(f\"{book}_{unit}\") for unit in units]\n",
    "    unit_floats.sort()\n",
    "    if unit_floats:\n",
    "        mid_index = len(unit_floats) // 2\n",
    "        part1 = unit_floats[:mid_index]\n",
    "        part2 = unit_floats[mid_index:]\n",
    "        if part1:\n",
    "            time_windows.append((part1[0], part1[-1]))\n",
    "        if part2:\n",
    "            time_windows.append((part2[0], part2[-1]))\n",
    "\n",
    "\n",
    "# Create lookup from unit_id -> (start, end)\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "                \n",
    "# Now `unit_interval_map` is defined and populated\n",
    "\n",
    "# ─── helper to export ANY nx.Graph to an HTML ───────────────────────────\n",
    "def export_pyvis(G, html_path, height=\"800px\", width=\"100%\"):\n",
    "    \"\"\"\n",
    "    Takes a networkx.Graph `G` whose nodes have a 'color' attribute (hex string),\n",
    "    and writes an interactive HTML to `html_path`.\n",
    "    \"\"\"\n",
    "    net = Network(notebook=False, height=height, width=width)\n",
    "    net.from_nx(G)                  # PyVis will read G.nodes[n]['color']\n",
    "    net.toggle_physics(False)       # disable physics for a static layout, optional\n",
    "    net.write_html(html_path)       # writes the interactive HTML\n",
    "    \n",
    "#----------- Unit-level graph construction (process each file/unit)\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        unit_graph = nx.Graph()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        # Add nodes with topic and filename info\n",
    "        for word in unique_words:\n",
    "            unit_graph.add_node(\n",
    "                word,\n",
    "                source=unit_id,\n",
    "                file_name=name_no_ext  # add full file name without extension\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    unit_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        unit_graphs[unit_id] = unit_graph\n",
    "\n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "                    \n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "global_graph = nx.Graph()\n",
    "\n",
    "# Add nodes and edges for global graph\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# add edges \n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Add dynamic 'start'for timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        start_val = unit_id_to_float(unit_id)\n",
    "        global_graph.nodes[word]['start'] = start_val\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "#Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Define the same palette you used in matplotlib:\n",
    "hex_palette = [\n",
    "    \"#e6194b\", \"#3cb44b\", \"#ffe119\", \"#4363d8\",\n",
    "    \"#f58231\", \"#911eb4\", \"#46f0f0\", \"#f032e6\",\n",
    "    \"#bcf60c\", \"#fabebe\", \"#008080\", \"#e6beff\",\n",
    "    \"#9a6324\", \"#fffac8\", \"#800000\", \"#aaffc3\",\n",
    "]\n",
    "topic_colors = {tid: hex_palette[tid] for tid in range(len(hex_palette))}\n",
    "\n",
    "def annotate_with_colors(G):\n",
    "    for n, data in G.nodes(data=True):\n",
    "        topic = data.get('topic')\n",
    "        # fallback if something went wrong\n",
    "        hexcol = topic_colors.get(topic, \"#CCCCCC\")\n",
    "        # 2) regular attribute:\n",
    "        G.nodes[n]['color'] = hexcol\n",
    "        # 3) viz:color block for GEXF viewers:\n",
    "        r, g, b = (int(hexcol.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        G.nodes[n]['viz'] = {'color': {'r': r, 'g': g, 'b': b, 'a': 1.0}}\n",
    "\n",
    "# Apply to all your unit‐level graphs:\n",
    "for unit_id, unit_graph in unit_graphs.items():\n",
    "    for word in unit_graph.nodes:\n",
    "        unit_graph.nodes[word]['topic'] = word_to_topic.get(word, \"unkown\")\n",
    "        annotate_with_colors(unit_graph)\n",
    "    nx.write_gexf(unit_graph, os.path.join(unit_graphs_dir, f\"{unit_id}_graph.gexf\"))\n",
    "\n",
    "# …and to the global graph:\n",
    "annotate_with_colors(global_graph)\n",
    "nx.write_gexf(global_graph, os.path.join(output_directory, \"202505_global_metrics_network.gexf\"))\n",
    "\n",
    "\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "# Function to export graph data (nodes, edges) into CSV or JSON\n",
    "def export_graph_data(book_graph, book_name):\n",
    "    # Export nodes (word, topic, source, start time) to a DataFrame\n",
    "    node_data = []\n",
    "    for node, data in book_graph.nodes(data=True):\n",
    "        node_info = {\n",
    "            'word': node,\n",
    "            'topic': data.get('topic', 'unknown'),\n",
    "            'source': data.get('source', 'unknown'),\n",
    "            'start': data.get('start', None)\n",
    "        }\n",
    "        node_data.append(node_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    node_df = pd.DataFrame(node_data)\n",
    "    node_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.csv\"), index=False)\n",
    "    node_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.json\"), orient='records', lines=True)\n",
    "    \n",
    "    # Export edges (word pairs, co-occurrence weight) to a DataFrame\n",
    "    edge_data = []\n",
    "    for u, v, data in book_graph.edges(data=True):\n",
    "        edge_info = {\n",
    "            'word_1': u,\n",
    "            'word_2': v,\n",
    "            'weight': data.get('weight', 0)\n",
    "        }\n",
    "        edge_data.append(edge_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    edge_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_edges.csv\"), index=False)\n",
    "    edge_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_edges.json\"), orient='records', lines=True)\n",
    "\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "\n",
    "# ---------------- Book-level graph construction (process each book)\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Compute topic distribution for the unit\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "\n",
    "       # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(set(word_to_topic.values())):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        unit_topic_rows.append(row)\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            if \"_\" in str(source):\n",
    "                start_val = unit_id_to_float(source)\n",
    "                start = start_val\n",
    "            else:\n",
    "                start = None\n",
    "\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=str(source),\n",
    "                start=start,\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # Perform community detection for the book\n",
    "    book_partition = community_louvain.best_partition(book_graph)\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "\n",
    "    print(f\"📖 Book {book} - Modularity Score: {book_modularity:.3f}\")\n",
    "\n",
    "    # Store results\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "\n",
    "    # Save book graph\n",
    "    book_gexf_filename = os.path.join(save_directory, f\"book_{book}.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf_filename)\n",
    "    print(f\"📂 Saved book graph: {book_gexf_filename}\")\n",
    "\n",
    "# Step 6: Export data\n",
    "# Export word-to-topic mapping\n",
    "with open(os.path.join(export_dir, \"20250425word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported word_to_topic.json\")\n",
    "\n",
    "\n",
    "# 2. Export token counts per book (word frequency for each book)\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)  # unit_id -> list of tokens\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Record words at unit level\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# Export token counts per book\n",
    "with open(os.path.join(export_dir, \"20250425token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 20250425token_counts_per_book.json\")\n",
    "\n",
    "# 3. Export unit tokens (words per unit)\n",
    "with open(os.path.join(export_dir, \"20250425unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 20250425unit_tokens.json\")\n",
    "\n",
    "# Step 6: Export the Global Graph (after all the book-level graphs)\n",
    "global_gexf_filename = os.path.join(save_directory, \"20250425_global_graph.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Export per-unit topic coverage (CSV format)\n",
    "unit_topic_csv = os.path.join(output_directory, \"20250425_w3_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "global_nodes = list(global_graph.nodes)\n",
    "#Count the degree of each node (i.e., how many edges are connected to it)\n",
    "node_degrees = {node: global_graph.degree(node) for node in network_core_vocab}\n",
    "\n",
    "# Filter nodes with degrees greater than 5\n",
    "filtered_nodes = [node for node, degree in node_degrees.items() if degree > 5]\n",
    "\n",
    "# Create a list of nodes with their degrees for the export\n",
    "export_vocab = [\n",
    "    {\"token\": node, \"count\": node_degrees[node]} for node in filtered_nodes\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "export_path = os.path.join(export_dir, \"network_core_vocab.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "#save all nodes to json\n",
    "export_path = os.path.join(export_dir, \"global_nodes.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported global nodes to {export_path}\")\n",
    "\n",
    "# Step 7: Collect words by topic and print topic examples\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items():\n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "# Step 8: Save modularity scores (Book-level modularity)\n",
    "modularity_output_file = os.path.join(output_directory, \"2025_0425_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab606d9-39cc-4d20-9c2a-9cb22a27207c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Book 1 - Modularity Score (global): 0.290\n",
      "📂 Saved book graph with global topics: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\w3_book_1.gexf\n",
      "📖 Book 2 - Modularity Score (global): 0.218\n",
      "📂 Saved book graph with global topics: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\w3_book_2.gexf\n",
      "📖 Book 3 - Modularity Score (global): 0.164\n",
      "📂 Saved book graph with global topics: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\w3_book_3.gexf\n",
      "📖 Book 4 - Modularity Score (global): 0.210\n",
      "📂 Saved book graph with global topics: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\w3_book_4.gexf\n",
      "✅ Exported 20250513_w3_word_to_topic.json\n",
      "✅ Exported w5_token_counts_per_book.json\n",
      "✅ Exported 2ß250513_w5_unit_tokens.json\n",
      "📂 Saved global graph: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\2ß25ß513_w3_global_graph.gexf\n",
      "📄 Saved unit-level topic coverage CSV to C:\\Users\\emine\\try_env\\2025_02\\community_detection\\20250513_w3_topic_coverage.csv\n",
      "📤 Exported network-based core vocabulary to: C:\\Users\\emine\\try_env\\2025_02\\community_detection\\20250513_w3_network_core_vocab.json\n",
      "Exported global nodes to C:\\Users\\emine\\try_env\\2025_02\\community_detection\\20250513_w5_global_nodes.json\n",
      "\n",
      "🔍 Topic Assignment Examples:\n",
      "Topic  0: universal, city, ancient, gps, fireplace, explode, connect, celebrate, youth, smell\n",
      "Topic  1: chain, state, press, basket, bank, chair, following, gate, sacred, superhero\n",
      "Topic  2: alone, realistic, crush, term, sooo, team, proud, log, diamond, nature\n",
      "Topic  3: leave, alligator, engine, ship, owner, driver, goodnight, outdoors, commercial, climber\n",
      "Topic  4: later, category, journal, information, stew, forecast, award, wind, overweight, rescue\n",
      "Topic  5: create, hometown, casual, pocket, finish, secretary, running, heat, quota, disagreement\n",
      "Topic  6: wrote, bowl, bath, vegan, filling, granddad, breakfast, fine, campaign, leg\n",
      "Topic  7: married, sulk, racism, flame, react, pro, hink, love, cerebral, freedom\n",
      "Topic  8: schoolyard, grade, doctor, painful, memory, happy, treatment, wrist, shake, forgive\n",
      "Topic  9: law, bout, flush, baby, kick, paper, greedy, bubble, fantastic, stack\n",
      "Topic 10: millennium, guerrilla, spaghetti, saddle, snowmobile, florestan, horse, count, disco, republic\n",
      "Topic 11: pen, paintbox, earring, splash, pierced, ladder, vet, instrument, guinea, detention\n",
      "📂 Modularity scores saved in C:\\Users\\emine\\try_env\\2025_02\\community_detection\\20250513_w3_distributional_modularity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# adds topic distribution in numbers / percentages April 11\n",
    "# attempts to use community louvain in the global graph as well and calculate modularity only for the book ranges!\n",
    "# adds source more granular in unit form to dataframe (first appearance in the book) April 8\n",
    "# adds unit level networks for stats analysis April 25\n",
    "# adds debugging to network construction April 29\n",
    "\n",
    "import os\n",
    "import utils  # Import utility functions\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import community as community_louvain\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "\n",
    "#seeds random generators for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))  # Define stop_words\n",
    "\n",
    "def unit_id_to_float(unit_id):\n",
    "    book, unit = unit_id.split(\"_\")\n",
    "    return float(f\"{book}.{int(unit):02d}\")\n",
    "    \n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "BASE_DIR = r\"C:\\Users\\emine\\try_env\\2025_02\"\n",
    "directory = os.path.join(BASE_DIR, \"texts\")\n",
    "pattern = \"*.txt\"\n",
    "file_paths = glob.glob(os.path.join(directory, pattern))\n",
    "texts = utils.read_text_files(directory)\n",
    "export_dir = os.path.join(BASE_DIR, \"community_detection\")\n",
    "\n",
    "# -----------sets base directory, locates txt files and reads them into texts\n",
    "\n",
    "#----- other directories to set\n",
    "save_directory = os.path.join(BASE_DIR, \"community_detection\")\n",
    "output_directory = os.path.join(BASE_DIR, \"community_detection\")\n",
    "unit_graphs_dir = os.path.join(BASE_DIR, \"community_detection\")\n",
    "os.makedirs(unit_graphs_dir, exist_ok=True)\n",
    "#----- other directories to set\n",
    "\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "lists_directory = os.path.join(BASE_DIR, \"lists\")\n",
    "proper_nouns = utils.load_proper_nouns(os.path.join(lists_directory, \"mod_proper_nouns_misspelled.csv\"))\n",
    "headwords_one = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_1000.txt\"))\n",
    "headwords_two = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_2000.txt\"))\n",
    "headwords_three = utils.load_word_list(os.path.join(lists_directory, \"BNC-COCA_headwords_3000.txt\"))\n",
    "list1_words = utils.lemmatize_word_list(headwords_one)  # Initialize word_list with list1_words\n",
    "list2_words = utils.lemmatize_word_list(headwords_two)  # Initialize word_list with list2_words\n",
    "list3_words = utils.lemmatize_word_list(headwords_three)  # Initialize word_list with list3_words\n",
    "# ----------- loading headword  & proper-nouns lists, lemmatizing them for consistency\n",
    "\n",
    "\n",
    "#----------- Compute word frequencies\n",
    "#word_frequencies = utils.compute_word_frequencies(texts, proper_nouns, stop_words)\n",
    "#threshold = 100\n",
    "#high_frequency_words = [word for word, freq in word_frequencies.items() if freq > threshold]\n",
    "# Update the most common words to be excluded in the graphs by updating stop words!\n",
    "#stop_words.update(high_frequency_words)\n",
    "#----------- Compute word frequencies\n",
    "\n",
    "#print(f\"Words occurring more than {threshold} times:\")\n",
    "#print(high_frequency_words)\n",
    "\n",
    "#----------- # Initialize data structures\n",
    "snapshots = {}  \n",
    "global_cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "unit_graphs = defaultdict(dict)\n",
    "token_first_appearance = {}\n",
    "modularity_scores = {}\n",
    "\n",
    "\n",
    "# --- Define custom time windows for each book ---\n",
    "time_windows = []\n",
    "unit_interval_map = {}  # Initialize unit_interval_map\n",
    "unit_topic_rows = []  # For storing topic distribution per unit\n",
    "\n",
    "book_ranges = {\n",
    "    1: range(1, 19),  # Book 1: 1_1_More to 1_18_More\n",
    "    2: range(1, 21),  # Book 2: 2_1_More to 2_20_More\n",
    "    3: range(1, 14),  # Book 3: 3_1_More to 3_13_More\n",
    "    4: range(1, 14)   # Book 4: 4_1_More to 4_13_More\n",
    "}\n",
    "\n",
    "#----------- Populate snapshots with file paths\n",
    "for book, units in book_ranges.items():\n",
    "    snapshots[book] = []\n",
    "    for unit in units:\n",
    "        file_name = f\"{book}_{unit}_More.txt\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            snapshots[book].append(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "#----------- org text files by book and unit, stored in snapshots\n",
    "\n",
    "# Define time windows for each book\n",
    "for book, units in book_ranges.items():\n",
    "    unit_floats = [unit_id_to_float(f\"{book}_{unit}\") for unit in units]\n",
    "    unit_floats.sort()\n",
    "    if unit_floats:\n",
    "        mid_index = len(unit_floats) // 2\n",
    "        part1 = unit_floats[:mid_index]\n",
    "        part2 = unit_floats[mid_index:]\n",
    "        if part1:\n",
    "            time_windows.append((part1[0], part1[-1]))\n",
    "        if part2:\n",
    "            time_windows.append((part2[0], part2[-1]))\n",
    "\n",
    "\n",
    "# Create lookup from unit_id -> (start, end)\n",
    "for start, end in time_windows:\n",
    "    for book, units in book_ranges.items():\n",
    "        for unit in units:\n",
    "            uid = f\"{book}_{unit}\"\n",
    "            uid_float = unit_id_to_float(uid)\n",
    "            if start <= uid_float <= end:\n",
    "                unit_interval_map[uid] = (start, end)\n",
    "                \n",
    "# Now `unit_interval_map` is defined and populated\n",
    "\n",
    "#----------- Unit-level graph construction (process each file/unit)\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        unit_graph = nx.Graph()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        ## Get the unit ID from filename\n",
    "        filename = os.path.basename(file_path)          # \"1_4_More.txt\"\n",
    "        name_no_ext = os.path.splitext(filename)[0]     # \"1_4_More\"\n",
    "        parts = name_no_ext.split(\"_\")                  # ['1', '4', 'More']\n",
    "\n",
    "        unit_id = \"_\".join(os.path.basename(file_path).split(\"_\")[:2])\n",
    "\n",
    "        # Track first appearance by unit\n",
    "        for word in unique_words:\n",
    "            if word not in token_first_appearance:\n",
    "                token_first_appearance[word] = unit_id\n",
    "\n",
    "        # Add nodes with topic and filename info\n",
    "        for word in unique_words:\n",
    "            unit_graph.add_node(\n",
    "                word,\n",
    "                source=unit_id,\n",
    "                file_name=name_no_ext  # add full file name without extension\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    unit_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "        unit_graphs[unit_id] = unit_graph\n",
    "\n",
    "        # Update global co-occurrence counts\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if word_i != word_j:\n",
    "                    global_cooccurrence_counts[word_i][word_j] += adj_matrix[i, j]\n",
    "                    \n",
    "#-----------initialize global co-occurrence structure; populated as unit texts are processed\n",
    "global_graph = nx.Graph()\n",
    "\n",
    "# Add nodes and edges for global graph\n",
    "for word in sorted(global_cooccurrence_counts.keys()):\n",
    "    global_graph.add_node(word, file_name=name_no_ext, source=token_first_appearance.get(word, \"unknown\"))\n",
    "# add edges \n",
    "for word_i, neighbors in global_cooccurrence_counts.items():\n",
    "    for word_j, weight in neighbors.items():\n",
    "        if weight > 0:\n",
    "            global_graph.add_edge(word_i, word_j, weight=weight)\n",
    "\n",
    "# Add dynamic 'start'for timeline\n",
    "for word in global_graph.nodes:\n",
    "    unit_id = token_first_appearance.get(word)\n",
    "    if unit_id and isinstance(unit_id, str) and \"_\" in unit_id:\n",
    "        start_val = unit_id_to_float(unit_id)\n",
    "        global_graph.nodes[word]['start'] = start_val\n",
    "\n",
    "# Perform community detection\n",
    "global_partition = community_louvain.best_partition(global_graph)\n",
    "word_to_topic = {word: topic_id for word, topic_id in global_partition.items()}\n",
    "#Add topics to global graph nodes\n",
    "for word, topic in word_to_topic.items():\n",
    "    if word in global_graph.nodes:\n",
    "        global_graph.nodes[word]['topic'] = topic\n",
    "\n",
    "# Define the same palette you used in matplotlib:\n",
    "hex_palette = [\n",
    "    \"#e6194b\", \"#3cb44b\", \"#722dd0\", \"#4363d8\",\n",
    "    \"#f58231\", \"#cc3300\", \"#46f0f0\", \"#0fa9d0\",\n",
    "    \"#384a03\", \"#663300\", \"#008080\", \"#6e00b3\",\n",
    "    \"#9a6324\", \"#0fa929\", \"#800000\", \"#3333ff\",\n",
    "]\n",
    "topic_colors = {tid: hex_palette[tid] for tid in range(len(hex_palette))}\n",
    "\n",
    "def annotate_with_colors(G):\n",
    "    for n, data in G.nodes(data=True):\n",
    "        topic = data.get('topic')\n",
    "        # fallback if something went wrong\n",
    "        hexcol = topic_colors.get(topic, \"#CCCCCC\")\n",
    "        # 2) regular attribute:\n",
    "        G.nodes[n]['color'] = hexcol\n",
    "        # 3) viz:color block for GEXF viewers:\n",
    "        r, g, b = (int(hexcol.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        G.nodes[n]['viz'] = {'color': {'r': r, 'g': g, 'b': b, 'a': 1.0}}\n",
    "\n",
    "# Apply to all your unit‐level graphs:\n",
    "for unit_id, unit_graph in unit_graphs.items():\n",
    "    for word in unit_graph.nodes:\n",
    "        unit_graph.nodes[word]['topic'] = word_to_topic.get(word, \"unkown\")\n",
    "        annotate_with_colors(unit_graph)\n",
    "    nx.write_gexf(unit_graph, os.path.join(unit_graphs_dir, f\"{unit_id}_graph_w3.gexf\"))\n",
    "\n",
    "# …and to the global graph:\n",
    "annotate_with_colors(global_graph)\n",
    "nx.write_gexf(global_graph, os.path.join(output_directory, \"20250513_w3_globalmetrics_network.gexf\"))\n",
    "\n",
    "\n",
    "\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "# Function to export graph data (nodes, edges) into CSV or JSON\n",
    "def export_graph_data(book_graph, book_name):\n",
    "    # Export nodes (word, topic, source, start time) to a DataFrame\n",
    "    node_data = []\n",
    "    for node, data in book_graph.nodes(data=True):\n",
    "        node_info = {\n",
    "            'word': node,\n",
    "            'topic': data.get('topic', 'unknown'),\n",
    "            'source': data.get('source', 'unknown'),\n",
    "            'start': data.get('start', None)\n",
    "        }\n",
    "        node_data.append(node_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    node_df = pd.DataFrame(node_data)\n",
    "    node_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.csv\"), index=False)\n",
    "    node_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_nodes.json\"), orient='records', lines=True)\n",
    "    \n",
    "    # Export edges (word pairs, co-occurrence weight) to a DataFrame\n",
    "    edge_data = []\n",
    "    for u, v, data in book_graph.edges(data=True):\n",
    "        edge_info = {\n",
    "            'word_1': u,\n",
    "            'word_2': v,\n",
    "            'weight': data.get('weight', 0)\n",
    "        }\n",
    "        edge_data.append(edge_info)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV or JSON\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    edge_df.to_csv(os.path.join(EXPORT_DIR, f\"{book_name}_edges.csv\"), index=False)\n",
    "    edge_df.to_json(os.path.join(EXPORT_DIR, f\"{book_name}_edges.json\"), orient='records', lines=True)\n",
    "#! ----- add export for analyses May 3 -------------\n",
    "\n",
    "# ---------------- Book-level graph construction (process each book)\n",
    "graphs = {}\n",
    "for book, file_paths in snapshots.items():\n",
    "    book_graph = nx.Graph()\n",
    "    all_words_in_book = set()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Compute topic distribution for the unit\n",
    "        topic_counts = defaultdict(int)\n",
    "        total_words_with_topic = 0\n",
    "        for word in words:\n",
    "            topic = word_to_topic.get(word)\n",
    "            if topic is not None:\n",
    "                topic_counts[topic] += 1\n",
    "                total_words_with_topic += 1\n",
    "\n",
    "       # Extract unit_id and start/end times\n",
    "        filename = os.path.basename(file_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        parts = name_no_ext.split(\"_\")\n",
    "        unit_id = \"_\".join(parts[:2])\n",
    "        start_float, end_float = unit_interval_map.get(unit_id, (None, None))\n",
    "\n",
    "        row = {\"start\": start_float, \"end\": end_float}\n",
    "        for topic in sorted(set(word_to_topic.values())):\n",
    "            count = topic_counts.get(topic, 0)\n",
    "            percent = (count / total_words_with_topic * 100) if total_words_with_topic > 0 else 0.0\n",
    "            row[f\"topic_{topic}\"] = round(percent, 2)\n",
    "        unit_topic_rows.append(row)\n",
    "\n",
    "        window_size = 3\n",
    "        adj_matrix, unique_words = utils.build_cooccurrence_matrix(words, window_size)\n",
    "\n",
    "        for word in unique_words:\n",
    "            source = token_first_appearance.get(word, \"unknown\")\n",
    "            if \"_\" in str(source):\n",
    "                start_val = unit_id_to_float(source)\n",
    "                start = start_val\n",
    "            else:\n",
    "                start = None\n",
    "\n",
    "            book_graph.add_node(\n",
    "                word,\n",
    "                topic=word_to_topic.get(word, \"unknown\"),\n",
    "                source=str(source),\n",
    "                file_name=name_no_ext,\n",
    "                start=start,\n",
    "            )\n",
    "\n",
    "        for i, word_i in enumerate(unique_words):\n",
    "            for j, word_j in enumerate(unique_words):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    book_graph.add_edge(word_i, word_j, weight=adj_matrix[i, j])\n",
    "\n",
    "    # **Instead of** computing a new partition:\n",
    "    for n in book_graph.nodes:\n",
    "        # copy the global topic assignment\n",
    "        book_graph.nodes[n]['topic'] = word_to_topic.get(n, 'unknown')\n",
    "\n",
    "    # then color it with your helper:\n",
    "    annotate_with_colors(book_graph)\n",
    "\n",
    "    # build a dict of {node: global_topic} for exactly this book’s nodes\n",
    "    book_partition = { n: book_graph.nodes[n]['topic'] for n in book_graph.nodes }\n",
    "    \n",
    "    # compute modularity of that fixed assignment\n",
    "    book_modularity = community_louvain.modularity(book_partition, book_graph)\n",
    "    modularity_scores[book] = book_modularity\n",
    "    graphs[book] = book_graph\n",
    "    print(f\"📖 Book {book} - Modularity Score (global): {book_modularity:.3f}\")\n",
    "\n",
    "    # Save book graph\n",
    "    book_gexf = os.path.join(save_directory, f\"w3_book_{book}.gexf\")\n",
    "    nx.write_gexf(book_graph, book_gexf)\n",
    "    print(f\"📂 Saved book graph with global topics: {book_gexf}\")\n",
    "\n",
    "# Step 6: Export data\n",
    "# Export word-to-topic mapping\n",
    "with open(os.path.join(export_dir, \"w20250513_w3_word_to_topic.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_to_topic, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 20250513_w3_word_to_topic.json\")\n",
    "\n",
    "\n",
    "# 2. Export token counts per book (word frequency for each book)\n",
    "token_counts_per_book = defaultdict(lambda: defaultdict(int))  # book -> word -> count\n",
    "unit_tokens = defaultdict(list)  # unit_id -> list of tokens\n",
    "\n",
    "for book, file_paths in snapshots.items():\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        words = utils.filter_text(text, proper_nouns, stop_words)\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Record words at unit level\n",
    "        filename = os.path.basename(file_path)\n",
    "        unit_id = \"_\".join(os.path.splitext(filename)[0].split(\"_\")[:2])\n",
    "\n",
    "        unit_tokens[unit_id].extend(words)\n",
    "\n",
    "        # Update frequency count for the book\n",
    "        for word in words:\n",
    "            token_counts_per_book[book][word] += 1\n",
    "\n",
    "# Export token counts per book\n",
    "with open(os.path.join(export_dir, \"2ß25ß513_w3_token_counts_per_book.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(token_counts_per_book, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported w5_token_counts_per_book.json\")\n",
    "\n",
    "# 3. Export unit tokens (words per unit)\n",
    "with open(os.path.join(export_dir, \"20250513_w3_unit_tokens.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unit_tokens, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Exported 2ß250513_w5_unit_tokens.json\")\n",
    "\n",
    "# Step 6: Export the Global Graph (after all the book-level graphs)\n",
    "global_gexf_filename = os.path.join(save_directory, \"2ß25ß513_w3_global_graph.gexf\")\n",
    "nx.write_gexf(global_graph, global_gexf_filename)\n",
    "print(f\"📂 Saved global graph: {global_gexf_filename}\")\n",
    "\n",
    "# Export per-unit topic coverage (CSV format)\n",
    "unit_topic_csv = os.path.join(output_directory, \"20250513_w3_topic_coverage.csv\")\n",
    "all_topics = sorted(set(word_to_topic.values()))\n",
    "fieldnames = [\"start\", \"end\"] + [f\"topic_{topic}\" for topic in all_topics]\n",
    "\n",
    "with open(unit_topic_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in unit_topic_rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"📄 Saved unit-level topic coverage CSV to {unit_topic_csv}\")\n",
    "\n",
    "# Export global core vocabulary (nodes in the global graph)\n",
    "network_core_vocab = list(global_graph.nodes)\n",
    "global_nodes = list(global_graph.nodes)\n",
    "#Count the degree of each node (i.e., how many edges are connected to it)\n",
    "node_degrees = {node: global_graph.degree(node) for node in network_core_vocab}\n",
    "\n",
    "# Filter nodes with degrees greater than 5\n",
    "filtered_nodes = [node for node, degree in node_degrees.items() if degree > 5]\n",
    "\n",
    "# Create a list of nodes with their degrees for the export\n",
    "export_vocab = [\n",
    "    {\"token\": node, \"count\": node_degrees[node]} for node in filtered_nodes\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "export_path = os.path.join(export_dir, \"20250513_w3_network_core_vocab.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"📤 Exported network-based core vocabulary to: {export_path}\")\n",
    "\n",
    "#save all nodes to json\n",
    "export_path = os.path.join(export_dir, \"20250513_w5_global_nodes.json\")\n",
    "with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported global nodes to {export_path}\")\n",
    "\n",
    "# Step 7: Collect words by topic and print topic examples\n",
    "topic_examples = defaultdict(list)\n",
    "for word, topic in word_to_topic.items():\n",
    "    topic_examples[topic].append(word)\n",
    "\n",
    "# Print sample words per topic\n",
    "print(\"\\n🔍 Topic Assignment Examples:\")\n",
    "for topic_id, words in sorted(topic_examples.items()):\n",
    "    sampled_words = random.sample(words, min(10, len(words)))  # up to 10 per topic\n",
    "    print(f\"Topic {topic_id:2d}: {', '.join(sampled_words)}\")\n",
    "\n",
    "# Step 8: Save modularity scores (Book-level modularity)\n",
    "modularity_output_file = os.path.join(output_directory, \"20250513_w3_distributional_modularity_scores.csv\")\n",
    "with open(modularity_output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book\", \"Modularity Score\"])\n",
    "\n",
    "    for book, score in modularity_scores.items():\n",
    "        writer.writerow([book, score])\n",
    "\n",
    "print(f\"📂 Modularity scores saved in {modularity_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc5255-bf23-40ef-960f-dbe16c5280b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (try_env)",
   "language": "python",
   "name": "try_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
